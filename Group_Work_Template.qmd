---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Group Name's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: InputMonoCondensed
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import requests
import zipfile
import matplotlib.cm as cm
import matplotlib.pyplot as plt
from matplotlib.colors import TwoSlopeNorm
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import fcluster, linkage
from scipy.stats import entropy
from dateutil.relativedelta import relativedelta
from datetime import datetime
from PIL import Image
from io import BytesIO
from shapely.geometry import Point
from urllib.parse import urlparse
import base64  # Add this import at the top
import IPython.display as display
import random
from pyproj import Proj, Transformer #for the distance to midpoint
from scipy.stats import shapiro
import scipy.stats as stats
import seaborn as sns
import statsmodels.api as sm
import statsmodels.stats.api as sms
from statsmodels.stats.api import het_breuschpagan
import esda
from splot.esda import moran_scatterplot
from splot.esda import plot_local_autocorrelation
from scipy.stats import skew


pd.set_option('display.max_columns', None)
pd.options.display.max_colwidth = 200
```

```{python}
#| echo: false
def cache_data(src:str) -> str:
    """Downloads and caches a remote file locally.
    src : str - The remote *source* for the file, any valid URL should work.
    dest : str - The *destination* location to save the downloaded file.    
    Returns a string representing the local location of the file.
    """
    dest = os.path.join('data','raw') #destination to save data
    
    url = urlparse(src) # We assume that this is some kind of valid URL 
    fn = (url.path.split('/')[-3])+(url.path.split('/')[-1]) # return the file name with date
    dfn = os.path.join(dest,fn) # Destination filename
    print(f"Writing to: {fn}")

    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:
        
        print(f"{dfn} not found or corrupted, downloading!")
        path = os.path.split(dest)
        if len(path) >= 1 and path[0] != '':
            os.makedirs(os.path.join(*path), exist_ok=True)
            # Download and save the file #os.path.join(*path)
            try:
                with open(dfn, "wb") as file:
                    response = requests.get(src)
                    response.raise_for_status()  # Raise an exception for HTTP errors
                    file.write(response.content)
                print("\tDone downloading.")
            except Exception as e:
                print(f"Error: {e}")
                
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)")
    else:
        print(f"Found {dfn} locally!")
        f_size = os.stat(dfn).st_size
        print(f"\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)")
        
    return dfn
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Set download URL for London data the date should be choosen by reviewing 
# the InsideAirbnb Get Data page and identifying the date of the required data
# We are using the 2 datasets with earliest and latest scrape of those available
#_____________________________________________________________________________________________________________________________
airbnbdates  = ["2023-12-10", "2024-09-06"]
airbnbcols = ['id', 'listing_url', 'picture_url', 'host_id', 'host_listings_count',
              'host_total_listings_count', 'property_type', 'room_type', 'price', 
              'minimum_nights', 'maximum_nights', 'availability_365', 
              'number_of_reviews', 'latitude', 'longitude', 'last_review', 
              'beds', 'bedrooms']
airbnbdfs = []
for date in airbnbdates:
    airbnburl  = f"https://data.insideairbnb.com/united-kingdom/england/london/{date}/data/listings.csv.gz"
    dfn = cache_data(airbnburl)
    airbnbdfs.append(pd.read_csv(dfn, low_memory=False,usecols=airbnbcols))
   
airbnb_dflist = [airbnb_old, airbnb_new] = airbnbdfs #airbnb_old is the earlier scrape, and new is the later scrape. We don't hardcode the dates for reproducibility with other dates and times.

airbnb_gdfs = []
for df in airbnb_dflist:
#_____________________________________________________________________________________________________________________________
#   I convert the df into gdf, reproject into EPSG 27700
#_____________________________________________________________________________________________________________________________  
    df = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs='EPSG:4326')
    
    df = df.set_crs('EPSG:4326', allow_override=True)
    df.to_crs('EPSG:27700', inplace=True)
    airbnb_gdfs.append(df)
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# InsideAirbnb data cleaning. We first define the functions to be used in cleaning.
#_____________________________________________________________________________________________________________________________
def pick_active_listings(df, date='str', col="str"):
    """
    Converting review column to datetime, and selecting only the recently active listings. 
    Recently active is defined as those which received at least one review for the year precedeng the scrape.
    Arguments:
    date: string, format "%Y-%m-%d"
    dataframe
    col: string, colname with "last_review" or its alternative
    """
    df.loc[:, col] = pd.to_datetime(df[col])
    date = datetime.strptime(date, "%Y-%m-%d")
    df = df[df['last_review'] > (date - relativedelta(years=1))] #| (df['last_review'].isnull())]
    
    print(f"Data frame after filtering last review date is {df.shape[0]:,} x {df.shape[1]} long.")
    return df
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Why do we remove duplicates?
# In the regression part of our analysis, we  want to calculate N of homes that are somehow involved in Airbnb listings, and therefore don't fulle realise it's potential on traditional rental/housing market.
# If we took just listing locations, with the amount of duplicates that we found, the N would show ACTIVITY of hosts more than loss of homes for housing market.
# ChatGPT was used here to determine clustering methods, and for debugging.
#_____________________________________________________________________________________________________________________________

def find_duplicates(df_valid):
    """
    The function looks at hosts with multiple listings and checks if they are within 300m radius (150m max location scattering as per Airbnb's anonymisation algorithm, x2).
    It then estimates the number of genuine homes within the cluster.
    
    Returns a gdf with new columns: 
    'cluster_id' - each unique value is a cluster
    'easting', 'northing' - used in calculating the proximity of listings
    'prvt_rms_in_cluster','entr_hms_in_cluster' - summarises N of private rooms and entire homes within the cluster
    'genuine', 'true_nrooms' - boolean column, and int column, with the following assumptions:

    If the cluster has 0 entire homes, and N rooms, first room will be "converted" to an entire home, and the rest will be discarded. 
    The location of the point will be amended to represent centrepoint of all rooms in the cluster.
    Record of the N of rooms will stay in col "true_nrooms"
    If the cluster has rooms and entire homes, the homes will be treated as genuine homes, and rooms treated as duplicates of the genuine home. 
    With the current timeframes, further enquiries into uniqueness of homes vs rooms inside each cluster is not feasible.
    """
    
    df['easting'], df['northing'] = df.geometry.x, df.geometry.y #this is needed for the clustering  
#_____________________________________________________________________________________________________________________________
#   We are looking for duplicates among multi-listing hosts. First, we filter by new_listings_count > 1.
#_____________________________________________________________________________________________________________________________
    finddups = df_valid[df_valid['new_host_listings_count'] > 1]
    print(f"There are {finddups.shape[0]:,} listings from multi-listing hosts.")
#_____________________________________________________________________________________________________________________________
#   I perform cluster analysis a1nd mark listings from the same host that are within 300m from each other 
#   (150m max location scattering as per Airbnb's anonymisation algorithm, x2). They constitute a cluster, and we then assess if they are a duplicate or not.
#   The selected method computes pairwise distances that can calculate the distance matrix between all points in a group. 
#   Before this method I tried KNN, but the listings were paired incorrectly. 
#   cdist solved this, and it is relatively light on small groups (our dataset is large, but it is split into small groups - by host).
#
#   It is possible that listings outside clusters also could be in one entire home. E.g. there could be several hosts registered for one home.
#   We acknowledge this gap, and advise this for further research.
#_____________________________________________________________________________________________________________________________
    # Process each host group
    for host_id, group in finddups.groupby("host_id"):
        if len(group) > 1:  # Only proceed if there is more than 1 listing
            coords = group[['easting', 'northing']].to_numpy()
            
            # Calculate pairwise distances using pdist (returns condensed distance matrix)
            dist_matrix = pdist(coords)
            
            # Perform hierarchical clustering using the condensed distance matrix
            linkage_matrix = linkage(dist_matrix, method='single')
            clusters = fcluster(linkage_matrix, t=300, criterion='distance')
            
            # Create a series to count cluster sizes
            cluster_sizes = pd.Series(clusters).value_counts()
    
            # Assign clusters to rows in the group, but only for clusters with more than one member - these will retain default column value of None
            finddups.loc[group.index, 'cluster_id'] = [
                f"{host_id}-{cluster}" if cluster_sizes[cluster] > 1 else None
                for cluster in clusters
            ]
#_____________________________________________________________________________________________________________________________
#   We create a subset with non-unique listings only to calculate true_nrooms and determine genuineness.
#   At the very end of the function, this subset is merged back into df_valid.
#_____________________________________________________________________________________________________________________________
    finddups1 = finddups[~finddups['cluster_id'].isna()]
    finddups1 = finddups1.loc[finddups.duplicated(subset='cluster_id', keep=False)]   
    print(f"Found {len(set(finddups1.cluster_id)):,} clusters.")   
#_____________________________________________________________________________________________________________________________
#   I calculate N of entire homes and rooms in each cluster.
#   Limitation found during EDA: "bedrooms" column in InsideAirbnb differs between scrapes. 
#   "2023-12-10" scrape has NaN values in this column, while "2024-09-06" scrape has full data.
#   For both datasets, we use "beds" column as a substitute, because airbnb2024.bedrooms.median()/airbnb2024.beds.median() = 1.
#_____________________________________________________________________________________________________________________________
    finddups1 = finddups.loc[finddups.duplicated(subset='cluster_id',keep=False)]
#   Loop through each cluster by cluster_id. 
    for i, group in finddups1.groupby("cluster_id"):
        # Count number of entire homes and private rooms in the group
        n_entire_homes = group[group['room_type'] == 'Entire home/apt'].shape[0]
        n_private_rooms = group[group['room_type'] == 'Private room'].shape[0]   
        # Assign these counts back to the original DataFrame
        finddups1.loc[group.index, 'entr_hms_in_cluster'] = n_entire_homes
        finddups1.loc[group.index, 'prvt_rms_in_cluster'] = n_private_rooms          
#_____________________________________________________________________________________________________________________________
#   I then determine whether listings are genuine unique homes, or rooms from one entire home.
#   If the cluster has 0 entire homes, and N rooms, first room will be "converted" to an entire home, and the rest will be discarded.
#   Record of the N of rooms will stay in col "true_nrooms".
#   If the cluster has rooms and entire homes, the homes will be treated as genuine homes, and rooms treated as duplicates of the genuine home.
#   It is possible that all entire homes in one cluster are absolutely identical, but with the methods employed we can go this far. We acknowledge the gap and suggest it for further research.
#_____________________________________________________________________________________________________________________________
    for cluster_id, group in finddups1.groupby("cluster_id"):
        
        num_entire_homes = (group['room_type'] == 'Entire home/apt').sum()
        if num_entire_homes == 0:
            # If only rooms are in the cluster
            finddups1.loc[group.index, 'genuine'] = False  # Mark all as duplicates
            finddups1.loc[group.index[0], 'genuine'] = True  # First room becomes the genuine home
#_____________________________________________________________________________________________________________________________
#           For remaining valid homes the geometry is replaced with centroid x and y, taken from points in cluster.  
#           Without this the point and all associated rooms can be aggregated by different spatial unit, considering the scale of the location approximation (up to 150m from origin).
#           This is not crucial for our research, because we aggregate by borough, and potential aggregation errors will be minimal. But for other research with smaller geospatial units it will prove more useful.
#_____________________________________________________________________________________________________________________________
            dissolved_geometry = group.geometry.union_all()  # Combine all geometries in the group
            centroid = dissolved_geometry.centroid # Get the centroid of the combined geometry
            finddups1.at[group.index[0], 'geometry'] = Point(centroid.x, centroid.y)
        else:
            # Cluster has both rooms and entire homes
            finddups1.loc[group.index, 'genuine'] = False  # Default all to duplicates
            entire_home_indices = group[group['room_type'] == 'Entire home/apt'].index
            finddups1.loc[entire_home_indices, 'genuine'] = True  # Mark entire homes with true

    print(f"The clustering exercise identified {len(finddups1[finddups1['genuine'] == False])} listings that are potentially duplicates of other listings.")
    df_valid.update(finddups1)
    
    return df_valid
```

```{python}
#| echo: false
airbnb_gdf_clean = []
airbnb_gdf_ingenuine = []
airbnb_gdf_all = []

for df in airbnb_gdfs:
    print(f"Initial dataframe is {df.shape[0]:,} x {df.shape[1]} long.")
    df['price'] = df['price'].fillna('0').str.replace('$', '', regex=False).str.replace(',', '').astype(float).astype(int)
    df.drop(df[df["price"]==0].index, axis=0, inplace=True)
    print(f"Data frame after filtering invalid prices is {df.shape[0]:,} x {df.shape[1]} long.")
    
    df = pick_active_listings(df, date, "last_review")

    ints  = ['id', 'host_listings_count', 'host_id', 'bedrooms', 'beds', 
             'host_total_listings_count', 'minimum_nights', 'maximum_nights', 'availability_365', 'number_of_reviews']
    for i in ints:
        #print(f"Converting {i}")
        try:
            df.loc[:, i] = df[i].fillna(0).astype(float).astype(int)
        except ValueError as e:
            #print("  - !!!Converting to unsigned 16-bit integer!!!")
            df.loc[:, i] = df[i].fillna(0).astype(float).astype(pd.UInt16Dtype())
    #_____________________________________________________________________________________________________________________________
    # After these transformations, the N of listings for each host has changed. We dropped rows where listings were likely inactive at the time of the scrape.
    # We create a new column with host_listings_count, and calculate the counts using grouping by host_id.
    #_____________________________________________________________________________________________________________________________
    host_counts = df['host_id'].value_counts()
    df = pd.merge(df,host_counts,right_index=True,left_on='host_id').rename(columns={'count':'new_host_listings_count'})
        
    # null values in N of bedrooms will interfere with our analysis, therefore we drop them
    df = df[df['beds'].notna()]
    print(f"Data frame after cleaning invalid N of beds is {df.shape[0]:,} listings long.")
    
    # creating columns necessary for the analysis and filling with default values
    df['prvt_rms_in_cluster'] = df['beds'] #be are using beds for rooms, but for now just copy the column
    df['entr_hms_in_cluster'] = 0 #default will be 0
    df['cluster_id'] = None #will write ids here
    #df['true_nrooms'] = df['beds'] #estimation of true n of rooms proved inconclusive with the methods used. The parts related to rooms are commented out.
    df['genuine'] = True   
    
    dups = find_duplicates(df) 
    airbnb_gdf_all.append(dups)
    airbnb_gdf_ingenuine.append((dups[dups['genuine'] == False]).copy()) #collecting False for validation
    
    df = (df[df['genuine'] == True]).copy()
    df = df[(df['availability_365']) > (5.84*12)] #Filtering by "commercial" - rented out more than average per month for London. Average is 5.84 days

    print(f"Data frame after cleaning duplicate listings is {df.shape[0]:,} listings long (multi-listing hosts only).\n")
    print(f"Appending cleaned data frame to single-listing hosts data frame, and storing everything in list airbnb_gdf_clean.")
    
    airbnb_gdf_clean.append(df)
    print("Done.\n")
```

```{python}
#| echo: false
airbnb_gdf_clean[0].head(3) #this prints 1st element of the list, so the older dataset
#_____________________________________________________________________________________________________________________________
# At this point, airbnb_gdf_clean is a list, which contains [0] - clean dataset of older airbnb data, and [1] - clean newer data
#_____________________________________________________________________________________________________________________________
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
#   You might wonder, whether the find_duplicates function works properly.
#   We acknowledge that, given the timeframes, and considering that our research question is not specifically about duplicates,
#   it is not feasible to develop a complex validation mechanism.
#   However, you can see for yourself the results of the clustering and our assumptions, using the function below.
#_____________________________________________________________________________________________________________________________

# This was not developed as a proper validation tool, only as a proof of concept of finding duplicates.
# As this is not the main part of the exercise, ChatGPT was heavily used here to set up thumbnail plotting.

def check_genuine(df, **kwargs):
    """
    Receives df and random_state as args. If unspecified, gives random number as random_state.
    Displays the random ingenuine listing, and other listings from its cluster as clickable image thumbnails.
    """
    if not kwargs.get('random_state'):
        random_state = random.randint(0,1000)
    else:
        random_state = kwargs.get('random_state')
    print(f'Random_state unspecified, selecting:{random_state}.')
    
    # Select a random "ingenuine" listing from the dataset
    listing_check = df[df['genuine'] == False].sample(1, random_state=random_state)
    
    print(f"Let's have a look at the random listing. ID: {listing_check.id.values[0]}")
    print(f"Cluster number {listing_check.cluster_id.values[0]}")
    
    if (listing_check['beds'] > 4).any():
        print("Our estimate is that this might be a hostel, party home or similar hishly commercialised listing.")
    else:
        print("Our estimate is that this cluster might be one entire home.\n")
    
    print("All listings in this cluster are below. Click on the image to open the listings on website.")
    print("You might get inactive links with past datasets.")
    
    # Get all listings in the same cluster
    same_cluster = df[df["cluster_id"] == listing_check.cluster_id.values[0]]
    # print(same_cluster[["id", "listing_url"]])
    
    # Collect successful images and their associated URLs
    successful_images = []
    urls = []
    
    for idx, row in same_cluster.iterrows():
        try:
            response = requests.get(row['picture_url'], timeout=10)
            response.raise_for_status()  # Raise an error for HTTP issues
            img = Image.open(BytesIO(response.content))
            img.thumbnail((190, 190))  # Resize image for thumbnail display
            successful_images.append((img, row['listing_url']))
        except Exception as e:
            print(f"Failed to retrieve image for Listing ID: {row['id']}. Error: {e}")
    
    # If no successful images, return early
    if not successful_images:
        print("No images could be retrieved.")
        return
    
    # Render images and clickable links in Quarto Markdown
    html_content = "<table>"
    for i, (img, url) in enumerate(successful_images):
        if i % 3 == 0:  # Start a new row every 3 images
            html_content += "<tr>"
        
        # Convert the image to a data URL
        img_buffer = BytesIO()
        img.save(img_buffer, format="PNG")
        img_data = img_buffer.getvalue()
        img_base64 = base64.b64encode(img_data).decode('utf-8')  # Encoding the image
        img_src = f"data:image/png;base64,{img_base64}"
        
        # Add the image and link to the HTML
        html_content += f"""
        <td style="text-align:center; padding:5px">
            <a href="{url}" target="_blank"><img src="{img_src}" style="width:128px; height:auto; border:1px solid #ccc" /></a>
        </td>
        """
        if i % 3 == 2:  # End the row every 3 images
            html_content += "</tr>"
    
    html_content += "</table>"
    
    # Display the HTML in Quarto
    display.display(display.HTML(html_content))
```

```{python}
#| echo: false
check_genuine(airbnb_gdf_all[1]) #you can also define random state, e.g. check_genuine(airbnb_gdf_all[1],random_state=10)
#_____________________________________________________________________________________________________________________________
# Anna - this needs validation. Run this say 25 times and put how many results were correctly marked as dups
#_____________________________________________________________________________________________________________________________
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Loading other data - Csv
#_____________________________________________________________________________________________________________________________
raw_csvs_fn = ['Census_LSOA_Dwellings', 'Census_dwellings' 
    ,'Census_Households', 'Census_Tenure', 'Census_Accommodation', 
    'Census_BedroomOcc', 'Census_CarsVans', 'Census_Deprivation', 'Census_NsSeC', 
    'Census_PopDensity', 'Census_Addresses', 'Sales-2024-09', 
    'First-Time-Buyer-Former-Owner-Occupied-2024-09', 'New-and-Old-2024-09', 'onsrents']
csv_skiprows = [6, 6 ,6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, None]
csv_nrows = [4994, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 138543, 60219, 138543, 38390]
csv_parse_dates = [None, None, None, None, None, None, None, None, None, None, None, None, ['Date'], ['Date'], ['Time period']]
csv_usecols = [None, 
    ['local authority: county / unitary (as of April 2023)', 'mnemonic', 'Total'] 
    ,['mnemonic', '2021'],  
    ['mnemonic', 'Owned', 'Shared ownership', 'Social rented', 'Private rented or lives rent free'], 
    ['mnemonic', 'Detached', 'Semi-detached', 'Terraced', 'In a purpose-built block of flats or tenement','Part of a converted or shared house, including bedsits', 'Part of another converted building, for example, former school, church or warehouse', 'In a commercial building, for example, in an office building, hotel or over a shop'], 
    ['mnemonic', 'Occupancy rating of bedrooms: +2 or more', 'Occupancy rating of bedrooms: +1', 'Occupancy rating of bedrooms: 0', 'Occupancy rating of bedrooms: -1', 'Occupancy rating of bedrooms: -2 or less'],
    ['mnemonic', 'No cars or vans in household', '1 car or van in household', '2 cars or vans in household', '3 or more cars or vans in household'], 
    ['mnemonic', 'Household is not deprived in any dimension', 'Household is deprived in one dimension', 'Household is deprived in two dimensions', 'Household is deprived in three dimensions', 'Household is deprived in four dimensions'],
    ['mnemonic', 'L1, L2 and L3 Higher managerial, administrative and professional occupations', 'L4, L5 and L6 Lower managerial, administrative and professional occupations', 
    'L7 Intermediate occupations', 'L8 and L9 Small employers and own account workers', 'L10 and L11 Lower supervisory and technical occupations', 'L12 Semi-routine occupations', 
    'L13 Routine occupations', 'L14.1 and L14.2 Never worked and long-term unemployed', 'L15 Full-time students'], 
    ['mnemonic', '2021'], 
    ['mnemonic', 'Address one year ago is the same as the address of enumeration', 'Address one year ago is student term-time or boarding school address in the UK', 'Migrant from within the UK: Address one year ago was in the UK', 'Migrant from outside the UK: Address one year ago was outside the UK'],
    None,
    ['Date', 'Area_Code', 'First_Time_Buyer_Average_Price', 'Former_Owner_Occupier_Average_Price'],
    ['Date', 'Area_Code', 'New_Build_Average_Price', 'Existing_Property_Average_Price'], 
    None]

csv_dict = {
    "raw_csvs_fn": raw_csvs_fn,
    "csv_skiprows": csv_skiprows,
    "csv_nrows": csv_nrows,
    "csv_parse_dates": csv_parse_dates,
    "csv_usecols": csv_usecols,
}
data_df = []
for idx, item in enumerate(csv_dict["raw_csvs_fn"]):
    df = pd.read_csv(f"data/raw/{item}.csv", 
         skiprows = csv_dict["csv_skiprows"][idx], 
         nrows = csv_dict["csv_nrows"][idx], 
         usecols = csv_dict["csv_usecols"][idx] 
         ,parse_dates = csv_dict["csv_parse_dates"][idx]
    )
    data_df.append(df)
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Loading other data - Excel
#_____________________________________________________________________________________________________________________________
raw_excels_fn = [
    'vacant-dwellings-borough.xlsx','hpssadataset9medianpricepaidforadministrativegeographies.xls',
    'hpssadataset15lowerquartilepricepaidforadministrativegeographies.xls',
    'Detailed_LA_20232024.xlsx','privaterentalmarketstatistics231220.xls','privaterentalmarketstatistics231220.xls',
    'privaterentalmarketstatistics231220.xls','privaterentalmarketstatistics231220.xls'  
]
excel_skiprows = [1, 6, 6, 7, 6, 6, 6, 6]
excel_nrows = [34, 331, 331, 310, 357, 357, 357, 357]
excel_sheet_name = [ 2, 10, 10, 3, 12, 13, 14, 15]
excel_usecols = [
[0,21], [2,113], [2,113],[0,4,6,16],['Area Code1', 'Mean', 'Median'],
['Area Code1', 'Mean', 'Median'],['Area Code1', 'Mean', 'Median'],['Area Code1', 'Mean', 'Median']
]
excel_dict = {
    "raw_excels_fn": raw_excels_fn,
    "excel_skiprows": excel_skiprows,
    "excel_nrows": excel_nrows,
    "excel_sheet_name": excel_sheet_name,
    "excel_usecols": excel_usecols,
}
for idx, item in enumerate(excel_dict["raw_excels_fn"]):
    df = pd.read_excel(os.path.join('data','raw', f"{item}"), 
         skiprows = excel_dict["excel_skiprows"][idx], 
         nrows = excel_dict["excel_nrows"][idx], 
         usecols = excel_dict["excel_usecols"][idx] 
         ,sheet_name = excel_dict["excel_sheet_name"][idx]
    )
    first_col_name = df.columns[0]
    df.rename(columns={first_col_name: 'mnemonic'}, inplace=True)
    
    data_df.append(df)
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Drop NA and non-London rows. Filter the last step, because first item has codes by LSOA, not by borough
#_____________________________________________________________________________________________________________________________
for idx, df in enumerate(data_df):
    for col in df.columns:
        if isinstance(col, str) and pd.Series(col).str.contains('area.*code', case=False).any():
            df.rename(columns={col: 'mnemonic'}, inplace=True)  
    data_df[idx] = df[df['mnemonic'].notna()]

for idx, df in enumerate(data_df[1:], start=1):
    data_df[idx] = df[df['mnemonic'].str.startswith('E09', na=False)]
```

```{python}
#| echo: false
(df_censusLSOAdwell, df_censusdwellings, df_censusHHolds, df_censusTenure, df_censusaccomm, 
df_censusbedocc, df_censuscarsvans, df_censusdepriv, df_censusnssec, df_censuspopdens, df_censusaddress, df_hp_sales, df_hp_firsttime, 
df_hp_newbuild, df_rentalprice, df_vacantprops,df_hp_median,df_hp_lower, df_homless_A2P,df_PRS1bed,df_PRS2bed,df_PRS3bed,df_PRS4bed
) = data_df #We unpack the list, and assign new variable names
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Calculate entropy
#_____________________________________________________________________________________________________________________________
for_entropy = [df_censusTenure, df_censusaccomm, df_censusbedocc, df_censuscarsvans, df_censusdepriv, df_censusnssec, df_censusaddress]
for_entropy_strings = ['Tenure_entropy', 'Accommm_entropy', 'Bedocc_entropy', 'Cars_entropy', 'Depriv_entropy', 'NsSEC_entropy', 'Address_entropy']

for idx, i in enumerate(for_entropy):
    array = np.array(i.iloc[:, 1:])  # Select all rows and columns starting from the second column
    entropy_values = np.apply_along_axis(entropy, axis=1, arr=array)  # Apply entropy along each row (axis=1)
    i[f"{for_entropy_strings[idx]}"] = entropy_values
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Rename columns for clarity
#_____________________________________________________________________________________________________________________________
df_censusdwellings = df_censusdwellings.rename(
    columns={'local authority: county / unitary (as of April 2023)': 'LAName',
             'Total': 'Total_Dwellings'}
)
df_censusHHolds = df_censusHHolds.rename(columns={'2021': 'Total_Hholds'})
df_censuspopdens = df_censuspopdens.rename(columns={'2021': 'PopDensity'})
df_vacantprops = df_vacantprops.rename(columns={2023: '2023_LTVacant'})
df_hp_median = df_hp_median.rename(columns={'Year ending Mar 2023': 'MedianHP_2023'})
df_hp_lower = df_hp_lower.rename(columns={'Year ending Mar 2023': 'LowerQHP_2023'})

df_homless_A2P = df_homless_A2P.rename(columns={146430: 'HHoldsPrevDuty', 57340: 'HHoldEndAST', 37040: 'HHoldSellRelet'})

df_PRS1bed = df_PRS1bed.rename(columns={'Mean': 'MeanOneBedRent2023', 'Median': 'MedianOneBedRent2023'})
df_PRS2bed = df_PRS2bed.rename(columns={'Mean': 'MeanTwoBedRent2023', 'Median': 'MedianTwoBedRent2023'})
df_PRS3bed = df_PRS3bed.rename(columns={'Mean': 'MeanThreeBedRent2023', 'Median': 'MedianThreeBedRent2023'})
df_PRS4bed = df_PRS4bed.rename(columns={'Mean': 'MeanFourPlusBedRent2023', 'Median': 'MedianFourPlusBedRent2023'})
df_rentalprice = df_rentalprice.rename(columns={'Time period': 'Date'})
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Filter sales data for the finantial year
#_____________________________________________________________________________________________________________________________
time_fltr = [df_hp_sales, df_hp_firsttime, df_hp_newbuild, df_rentalprice]
for idx, i in enumerate(time_fltr):
    i["Date"] = pd.to_datetime(i["Date"], errors='coerce')
    time_fltr[idx] = i.query("Date >= '2023-04-01' and Date < '2024-03-31'")
df_hp_sales, df_hp_firsttime, df_hp_newbuild, df_rentalprice = time_fltr
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Calculate totals
#_____________________________________________________________________________________________________________________________
monthly_avg_rent = df_rentalprice.groupby(['mnemonic'])['Rental price'].mean().reset_index()
df_hp_firsttime_totals = df_hp_firsttime.groupby(['mnemonic']).mean(['First_Time_Buyer_Average_Price', 'Former_Owner_Occupier_Average_Price'])
df_hp_totals = df_hp_sales.groupby(['mnemonic']).sum(['Sales_Volume'])
df_hp_newbuild_totals = df_hp_newbuild.groupby(['mnemonic']).mean(['New_Build_Average_Price', 'Existing_Property_Average_Price'])

# df_homless_A2P.dropna(how='all', axis=1, inplace=True) 
# df_homless_A2P.dropna(how='all', axis=0, inplace=True) 
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Transfrom columns
#_____________________________________________________________________________________________________________________________
df_homless_A2P['HHoldsPrevDuty'] = pd.to_numeric(df_homless_A2P['HHoldsPrevDuty'], errors='coerce')
df_homless_A2P['HHoldEndAST'] = pd.to_numeric(df_homless_A2P['HHoldEndAST'], errors='coerce')
df_homless_A2P['HHoldSellRelet'] = pd.to_numeric(df_homless_A2P['HHoldSellRelet'], errors='coerce')
df_PRS3bed['MeanThreeBedRent2023'] = pd.to_numeric(df_PRS3bed['MeanThreeBedRent2023'], errors='coerce')
df_PRS3bed['MedianThreeBedRent2023'] = pd.to_numeric(df_PRS3bed['MedianThreeBedRent2023'], errors='coerce')
df_PRS4bed['MeanFourPlusBedRent2023'] = pd.to_numeric(df_PRS4bed['MeanFourPlusBedRent2023'], errors='coerce')
df_PRS4bed['MedianFourPlusBedRent2023'] = pd.to_numeric(df_PRS4bed['MedianFourPlusBedRent2023'], errors='coerce')
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Creating core datasets to be used in analysis: Housing data
#_____________________________________________________________________________________________________________________________
housing_merge = [df_censusdwellings, df_censusHHolds, df_vacantprops, df_hp_median,
                  df_hp_lower, df_hp_totals, df_homless_A2P, df_hp_firsttime_totals,
                  df_hp_newbuild_totals, df_censusTenure, monthly_avg_rent]

df_CoreHousing = housing_merge[0]
for df in housing_merge[1:]:
    df_CoreHousing = pd.merge(df_CoreHousing, df, on='mnemonic', how='outer')

df_CoreHousing['PrevDutyPercent'] = ((df_CoreHousing['HHoldsPrevDuty']) / (df_CoreHousing['Total_Hholds']))*100
df_CoreHousing['EndASTPercent'] = ((df_CoreHousing['HHoldEndAST']) / (df_CoreHousing['Total_Hholds']))*100
df_CoreHousing['SellReletPercent'] = ((df_CoreHousing['HHoldSellRelet']) / (df_CoreHousing['Total_Hholds']))*100
df_CoreHousing['VacantPercent'] = ((df_CoreHousing['2023_LTVacant']) / (df_CoreHousing['Total_Dwellings']))*100
df_CoreHousing['SalesPercent'] = ((df_CoreHousing['Sales_Volume']) / (df_CoreHousing['Total_Dwellings']))*100
#df_CoreHousing
```

```{python}
#df_censusdwellings['Total_Dwellings'].sum()
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Creating core datasets to be used in analysis: Demographics data
#_____________________________________________________________________________________________________________________________
demographics_merge = [df_censusdwellings, df_censusHHolds, df_censusTenure, df_censuspopdens,
                  df_censusaddress, df_censuscarsvans, df_censusdepriv, df_censusnssec,
                  df_censusbedocc, df_censusaccomm]

df_CoreDemographics = demographics_merge[0]
for df in demographics_merge[1:]:
    df_CoreDemographics = pd.merge(df_CoreHousing, df, on='mnemonic', how='outer')
#df_CoreDemographics
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Perform some spatial data wrangling on the airbnb data then load spatial data (boroughs, lsoa) and join the airbnb data to the spatial data.
#_____________________________________________________________________________________________________________________________
rents_merge = [df_PRS1bed, df_PRS2bed, df_PRS3bed, df_PRS4bed]
df_PrivateRentsRooms = rents_merge[0]
for df in rents_merge[1:]:
    df_PrivateRentsRooms = pd.merge(df_PrivateRentsRooms, df, on='mnemonic', how='outer')
#df_PrivateRentsRooms
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Performing spatial joins of Airbnb data by borough
#_____________________________________________________________________________________________________________________________

# Load London Borough and LSOA shapefiles - boroughs downloaded from "https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip"
boro_shp = os.path.join("data", "london-boundaries", "statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp")  
lsoa_shp = os.path.join("data", "lsoa-boundaries", "merged_lsoa_boundaries_london.shp")  

# read shapefile into gdf and reproject crs to 27700
boro_gdf = gpd.read_file(boro_shp).to_crs('EPSG:27700')[['NAME', 'GSS_CODE', 'ONS_INNER', 'geometry']]
lsoa_gdf = gpd.read_file(lsoa_shp).to_crs('EPSG:27700')[['lsoa21cd', 'lsoa21nm', 'lad22cd', 'lad22nm', 'geometry']]

# quickly plot the data
#boro_gdf.plot()
#lsoa_gdf.plot()
```

```{python}
airbnb_gdf_clean[1].columns
#len(airbnb_gdf_clean[1])
```

```{python}
#| echo: false
def airbnb_count(gdf, boro_gdf):
    """
    Function to count the airbnb locations within each borough and LSOA, split by room type.
    Args:
    gdf: airbnb gdf
    boro_gdf: geographic areas
    Both gdfs should be in the same projection.
    """ 
    #clip by London boundary
    gdf = gdf.clip(boro_gdf)
    
    # counts the airbnb locations within each borough, split by room type
    # set new gdf for each room type
    entire_ab_gdf = gdf[gdf['room_type'] == "Entire home/apt"]
    pr_ab_gdf = gdf[gdf['room_type'] == "Private room"]
    #other_ab_gdf = gdf[~gdf['room_type'].isin(["Entire home/apt", "Private room"])]
#_____________________________________________________________________________________________________________________________
#Due to the relatively low counts of "other" room types we can discount these from our analysis and focus 
#on "Entire Home/apts" and "Private rooms"
#_____________________________________________________________________________________________________________________________   
    # create point counts per borough
    c1 = boro_gdf.join(gpd.sjoin(gdf, boro_gdf).groupby("index_right").size().rename("total_airbnbs"))
    c2 = c1.join(gpd.sjoin(entire_ab_gdf, c1).groupby("index_right").size().rename("total_entireha"))
    boro_count_gdf = c2.join(gpd.sjoin(pr_ab_gdf, c2).groupby("index_right").size().rename("total_privateroom"))
    #boro_count_gdf = c3.join(gpd.sjoin(other_ab_gdf, c3).groupby("index_right").size().rename("total_other"))
    
    # replaces NaN values with zero
    ## could build funtion to iterate through this
    boro_count_gdf['total_entireha'] = boro_count_gdf['total_entireha'].fillna(0).astype('int64')
    boro_count_gdf['total_privateroom'] = boro_count_gdf['total_privateroom'].fillna(0).astype('int64')
    #boro_count_gdf['total_other'] = boro_count_gdf['total_other'].fillna(0).astype('int64')

    airbnb_count_boro = [boro_count_gdf, entire_ab_gdf, pr_ab_gdf
                         #,other_ab_gdf
                        ]
    
    return airbnb_count_boro
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Making the counts of airbnbs
#_____________________________________________________________________________________________________________________________
airbnb_2024_boro = airbnb_count(airbnb_gdf_clean[1], boro_gdf) #list of [boro_count_gdf - counts per borough, entire_ab_gdf - total counts, entire homes, pr_ab_gdf]
airbnb_2023_boro = airbnb_count(airbnb_gdf_clean[0], boro_gdf)
airbnb_2024_lsoa = airbnb_count(airbnb_gdf_clean[1], lsoa_gdf)
airbnb_2023_lsoa = airbnb_count(airbnb_gdf_clean[0], lsoa_gdf)
```

```{python}
# #| echo: false
len(airbnb_2024_boro[1]), len(airbnb_2023_boro[1]), len(airbnb_2024_lsoa[1]), len(airbnb_2023_lsoa[1])
```

```{python}
#| echo: false
#_____________________________________________________________________________________________________________________________
# Creating a basic map of airbnb locations by borough
# lsoa is not being plotted as at this scale it will overcomplicate the map
#_____________________________________________________________________________________________________________________________
fig, ax = plt.subplots(1,1, figsize=(8,6))

# Plot all borough and airbnb data to the same axes
boro_gdf.plot(edgecolor='red', facecolor='none', ax=ax)
airbnb_2024_boro[1].plot(edgecolor='blue', facecolor='none', ax=ax,alpha=0.25, markersize=1.0)
airbnb_2024_boro[2].plot(edgecolor='green', facecolor='none', ax=ax, alpha=0.25, markersize=1.0)
#airbnb_2024_boro[3].plot(edgecolor='orange', facecolor='none', ax=ax, alpha=0.25, markersize=1.0)

# Set the x and y limits
ax.set_xlim(501000, 563000)
ax.set_ylim(155000, 202000)

# Save the image (dpi is 'dots per inch')
#os.makedirs('img', exist_ok=True)
#plt.savefig(os.path.join('img','borough_map_with_air_bnb_locations.png'), dpi=150)
```

```{python}
#| echo: false
print(f"We have identified {len(airbnb_gdf_clean[0])} valid listings in 2023, and {len(airbnb_gdf_clean[1])} valid listings in 2024.")
print(f"It accounted for {len(airbnb_gdf_clean[0])/(df_censusdwellings['Total_Dwellings'].sum()/100):.2f}% and {len(airbnb_gdf_clean[1])/(df_censusdwellings['Total_Dwellings'].sum()/100):.2f}% of total dwellings respectively. \nThe number of dwellings is based on 2021 Census.\n")
print(f"This does not consider 2024 completions.")

print(f"In 2023, there were:")
print(airbnb_gdf_clean[0]['room_type'].value_counts())
print(f"{len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] == 1])} hosts with one listing, and {len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] > 1])} hosts with multiple listings.")
print(f"\nIn 2024, there were:")
print(airbnb_gdf_clean[1]['room_type'].value_counts())
print(f"{len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] == 1])} hosts with one listing, and {len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] > 1])} hosts with multiple listings.")
print("\nDue to the relatively low counts of “other” room types we can discount these from our analysis and focus on “Entire Home/apts” and “Private rooms”.")
```

```{python}
#| echo: false
def rel_prcnt_change_gdfs(gdf_old, gdf_new, cols):
    """
    Function to find different in number of listings between 2 dates.
    Args:
    gdf_old: earlier airbnb dataset
    gdf_new: latest airbnb dataset
    cols: specify a subset of columns to be substracted
    """
    #Verify that the geometry matches
    if not gdf_old.geometry.equals(gdf_new.geometry):
        raise ValueError("The geometries of the two GeoDataFrames do not match!")
    
    #Verify that the specified columns exist in both GeoDataFrames
    for col in cols:
        if col not in gdf_old.columns or col not in gdf_new.columns:
            raise ValueError(f"Column '{col}' does not exist in both GeoDataFrames.")
    
    #Create a copy of the new gdf to store the differences
    gdf_diff = gdf_new.copy()
    
    # Subtract the specified columns
    for col in cols:
        gdf_diff[col] = ((gdf_new[col] - gdf_old[col]) / gdf_old[col])*100
    
    # Ensure the geometry column is preserved
    gdf_diff['geometry'] = gdf_new['geometry']
    
    return gdf_diff
```

```{python}
#| echo: false
cols_diff_airbnb = ['total_airbnbs', 'total_entireha', 'total_privateroom']
diff2024_2023_boro = rel_prcnt_change_gdfs(airbnb_2023_boro[0], airbnb_2024_boro[0], cols_diff_airbnb)
diff2024_2023_lsoa = rel_prcnt_change_gdfs(airbnb_2023_lsoa[0], airbnb_2024_lsoa[0], cols_diff_airbnb)
```

```{python}
#| echo: false
diff2024_2023_boro.head(2)
```

```{python}
#| echo: false
diff2024_2023_boro.head(2)
```

```{python}
#| echo: false
diff2024_2023_boro["total_airbnbs"].hist()
```

## Map Borough data

```{python}
#| echo: false
# Determine the min and max values across both columns
vmin = min(diff2024_2023_boro['total_entireha'].min(), diff2024_2023_boro['total_privateroom'].min())
vmax = max(diff2024_2023_boro['total_entireha'].max(), diff2024_2023_boro['total_privateroom'].max())

# Create a TwoSlopeNorm instance
norm = TwoSlopeNorm(vmin=-100, vcenter=0, vmax=100)

# Create the subplots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Plot 'total_entireha' with shared color scale and custom normalization
diff2024_2023_boro.plot(column='total_airbnbs', ax=axes[0], legend=True,
                        legend_kwds={'label': "Relative Percentage Difference in Points by Area, %",
                                     'orientation': "horizontal"},
                        cmap='coolwarm', edgecolor='black',
                        norm=norm)  # Apply custom normalization

# Add title and labels
axes[0].set_title("Relative Percentage Difference in count of the AirBnb locations available (total count) by Borough, 2024-2023", fontsize=8)
axes[0].set_xlabel("Longitude")
axes[0].set_ylabel("Latitude")

# Plot 'total_privateroom' with shared color scale and custom normalization
diff2024_2023_boro.plot(column='total_entireha', ax=axes[1], legend=True,
                        legend_kwds={'label': "Relative Percentage Difference in Points by Area, %",
                                     'orientation': "horizontal"},
                        cmap='coolwarm', edgecolor='black',
                        norm=norm)  # Apply custom normalization

# Add title and labels
axes[1].set_title("Relative Percentage Difference in count of the AirBnb locations available as \"entire home\" rentals by Borough, 2024-2023", fontsize=8)
axes[1].set_xlabel("Longitude")
axes[1].set_ylabel("Latitude")

# # Plot 'total_privateroom' with shared color scale and custom normalization
# diff2024_2023_boro.plot(column='total_privateroom', ax=axes[1], legend=True,
#                         legend_kwds={'label': "Relative Percentage Difference in Points by Area, %",
#                                      'orientation': "horizontal"},
#                         cmap='coolwarm', edgecolor='black',
#                         norm=norm)  # Apply custom normalization

# # Add title and labels
# axes[1].set_title("Relative Percentage Difference in count of the AirBnb locations available as \"private room\" rentals by Borough, 2024-2023", fontsize=8)
# axes[1].set_xlabel("Longitude")
# axes[1].set_ylabel("Latitude")

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

print(f"The number of entire homes per borough grew more, than the overall number of listings.")
```

```{python}
#| echo: false
# Determine the min and max values across both columns
vmin = min(diff2024_2023_lsoa['total_entireha'].min(), diff2024_2023_lsoa['total_privateroom'].min())
vmax = max(diff2024_2023_lsoa['total_entireha'].max(), diff2024_2023_lsoa['total_privateroom'].max())

# Create a TwoSlopeNorm instance
norm = TwoSlopeNorm(vmin=-65, vcenter=0, vmax=65)

# Create the subplots
fig, axes = plt.subplots(1, 2, figsize=(15, 8))

# Plot 'total_entireha' with shared color scale and custom normalization
diff2024_2023_lsoa.plot(column='total_entireha', ax=axes[0], legend=True,
                        legend_kwds={'label': "Relative Percentage Difference in Points by Area, %",
                                     'orientation': "horizontal"},
                        cmap='coolwarm', edgecolor='white', linewidth=0.1,
                        norm=norm)  # Apply custom normalization

# Add title and labels
axes[0].set_title("Relative Percentage Difference in count of the AirBnb locations available as \"Entire home/apt\" rentals by LSOA, 2024-2023", fontsize=8)
axes[0].set_xlabel("Longitude")
axes[0].set_ylabel("Latitude")

# Plot 'total_privateroom' with shared color scale and custom normalization
diff2024_2023_lsoa.plot(column='total_privateroom', ax=axes[1], legend=True,
                        legend_kwds={'label': "Relative Percentage Difference in Points by Area, %",
                                     'orientation': "horizontal"},
                        cmap='coolwarm', edgecolor='white', linewidth=0.1,
                        norm=norm)  # Apply custom normalization

# Add title and labels
axes[1].set_title("Relative Percentage Difference in count of the AirBnb locations available as \"private room\" rentals by LSOA, 2024-2023", fontsize=8)
axes[1].set_xlabel("Longitude")
axes[1].set_ylabel("Latitude")

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

print(f"While number of private rooms dropped, the number of entire homes increased considerably. It is possible that the flats with rooms on airbnb converted to entire homes listings.")
```

```{python}
# #| echo: false
# # write the data to be used in further analysis (ONLY IF NEEDED. CODE WORKS WITHOUT THIS)
# fn = "borough_counts.csv"
# path = os.path.join('data','clean')

# if not os.path.exists(path):
#     print(f"Creating {path} under {os.getcwd()}")
#     os.makedirs(path)
    
# boro_count_gdf.to_csv(os.path.join(path,fn), index=False)
# #boro_count_gdf.to_file("boro_count.geoparquet")
# print("Done.")

# fn = "lsoa_counts.csv"
# path = os.path.join('data','clean')

# if not os.path.exists(path):
#     print(f"Creating {path} under {os.getcwd()}")
#     os.makedirs(path)
    
# lsoa_count_gdf.to_csv(os.path.join(path,fn), index=False)
# #lsoa_count_gdf.to_file("lsoa_count.geoparquet")
# print("Done.")
```

## Ordinary Least Squares Regression

```{python}
#___________________ extracting data needed for regression _______________
# Extracting the percentage and the total number of vacant properties in 2023
For_regressionV = df_CoreDemographics[['VacantPercent','2023_LTVacant', 'LAName', 'mnemonic']]

# Display the first few rows to ensure it worked correctly
print(For_regressionV.head())

#print(df_rentalprice) 

# Extracting Annual change column because we want to observe how the number of listings may influence the change in rental price
For_RegressionR = df_rentalprice[['mnemonic', 'Area name', 'Annual change']]

#print(For_RegressionR)

# ______________________ calculating the distance to the centre _________

# defining latitudes and longitude finder from london uk 
lat_lon = [
    (51.5014, -0.1419),  # Buckingham Palace
    (51.5081, -0.0760),  # Tower of London
    (51.5194, -0.1269),  # British Museum
    (51.5007, -0.1246),  # Big Ben
    (51.5033, -0.1195)   # London Eye
]

# Reprojecting coordinates to BNG (EPSG:27700)
transformer = Transformer.from_crs("epsg:4326", "epsg:27700", always_xy=True)
eastings_northings = [transformer.transform(lon, lat) for lat, lon in lat_lon]

# Calculating midpoint
eastings, northings = zip(*eastings_northings)
midpoint = (np.mean(eastings), np.mean(northings))


# Calculating centroids of each borough and distance to midpoint
boro_gdf["centroid"] = boro_gdf.geometry.centroid
boro_gdf["distance_to_midpoint"] = boro_gdf["centroid"].apply(
    lambda point: np.sqrt((point.x - midpoint[0])**2 + (point.y - midpoint[1])**2)
)


# Display only the NAME and distance_to_midpoint columns
#print(boro_gdf[['GSS_CODE', 'distance_to_midpoint', 'geometry']])

#print(diff2024_2023_boro)
```

```{python}
from scipy.stats import shapiro
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
import seaborn as sns


#shapiro wilk test to see if the data is normally distributed
#-------  on the listings data ---------

# Perform the Shapiro-Wilk test
stat, p_value = shapiro(diff2024_2023_boro["total_airbnbs"])

# Display the results
print(f"Shapiro-Wilk Test Statistic: {stat}")
print(f"P-value: {p_value}")


#the W values 0.7... is less than 1 (suggesting that the data is not normally distributed) and the p value 0.00000505 is <0.05
#therefore the null hypothesis which assumes that the data is normally distributed is rejected 



# ----------- Perform the test on the '2023' column ------ #
stat, p_value = shapiro(For_regressionV["VacantPercent"])
print(f"Shapiro-Wilk Test Statistic: {stat}")
print(f"P-value: {p_value}")

#Interpretation 
#0.9465112270729066 W value close to one so its close to normal 
#p value is 0.10520533110791175 greater than 0.05 so we can fail to reject the null hypothesis as there is no significant evidence
#that the data is not normally distributes



# Create Q-Q plot
stats.probplot(For_regressionV["VacantPercent"], dist="norm", plot=plt)
plt.show()

#hist to actually visualise the distribution to determine whether we transform or not
# Histogram with a density line
sns.histplot(For_regressionV["VacantPercent"], kde=True, color="blue", bins=20)

# Adding labels and title
plt.xlabel('vacant properties')
plt.ylabel('Density')
plt.title('Histogram vacant properties')

# Show plot
plt.show()

#as you can see it is still a bit skewed to the right



#Now we'll do one for the rental price
stat, p_value = shapiro(For_RegressionR['Annual change'])
print(f"Shapiro-Wilk Test Statistic: {stat}")
print(f"P-value: {p_value}")

#owned
stat, p_value = shapiro(boro_gdf['distance_to_midpoint'])
print(f"Shapiro-Wilk Test Statistic: {stat}")
print(f"P-value: {p_value}")


# checking ifits skewed
from scipy.stats import skew

skew_vac = skew(For_regressionV['VacantPercent'])


print(f"Skewness of Rented: {skew_vac}")

```

```{python}
# Histograms for all variables to visualise distribution 
# Histogram for "total_airbnbs"
sns.histplot(diff2024_2023_boro["total_airbnbs"], kde=True, color="blue", bins=20)
plt.xlabel('Total Airbnbs')
plt.ylabel('Density')
plt.title('Histogram of Total Airbnbs')
plt.show()

# Histogram for "VacantPercent"
sns.histplot(For_regressionV["VacantPercent"], kde=True, color="green", bins=20)
plt.xlabel('Vacant Percent')
plt.ylabel('Density')
plt.title('Histogram of Vacant Percent')
plt.show()

# Histogram for "Annual Change"
sns.histplot(For_RegressionR['Annual change'], kde=True, color="orange", bins=20)
plt.xlabel('Annual Change')
plt.ylabel('Density')
plt.title('Histogram of Annual Change')
plt.show()

# Histogram for "distance_to_midpoint"
sns.histplot(boro_gdf['distance_to_midpoint'], kde=True, color="purple", bins=20)
plt.xlabel('Distance to Midpoint')
plt.ylabel('Density')
plt.title('Histogram of Distance to Midpoint')
plt.show()
```

```{python}
#______ ________ merging the data for the regression model __________



# Merge using different column names
vacantprops_airbnb = pd.merge(For_regressionV[['mnemonic', 'LAName', 'VacantPercent']],  
                                   diff2024_2023_boro[['GSS_CODE', 'total_airbnbs']], 
                                   left_on='mnemonic',  
                                   right_on='GSS_CODE',  
                                   how='left', suffixes=('', '_dup'))


# Print the first few rows of the merged DataFrame
#print(vacantprops_airbnb.head())


#Adding rents
Adding_more = pd.merge(
    vacantprops_airbnb, 
    For_RegressionR[['mnemonic', 'Annual change']], 
    left_on='mnemonic',  # Merge on LAcode from vacantprops_with_tenure
    right_on='mnemonic',  # Merge on GSS_CODE from merged_borough_counts
    how='inner'  # Inner join to only keep matching rows
)

# Merging the result with boro to add Distance to Midpoint
Regression_Final = pd.merge(
    Adding_more, 
    boro_gdf[['GSS_CODE', 'distance_to_midpoint', 'geometry']], 
    on='GSS_CODE',  # Merge on GSS_CODE which is common
    how='inner'  # Inner join to keep only matching rows
)

# Display the final merged dataframe
#print(Regression_Final)


# Removing duplicate columns if any
Regression_Final = Regression_Final.loc[:, ~Regression_Final.columns.duplicated()]


# Dropping the redundant 'mnemonic' column after merging
Regression_Final1 = Regression_Final.drop(columns=['mnemonic'], errors='ignore')

# Print the final DataFrame
#print(Regression_Final1.head())

#______ grouping the rows by borough ______


Regression_Final = Regression_Final1.groupby('LAName').agg({
    'VacantPercent': 'mean',        # Example: Mean of VacantPercent
    'total_airbnbs': 'mean',         # Example: Sum of total_airbnbs
    'Annual change': 'mean',        # Example: Mean of Annual change
    'distance_to_midpoint': 'mean', # Example: Mean of distance_to_midpoint
}).reset_index()

# Print the grouped DataFrame
#print(Regression_Final)


Regression_Final1.loc[:, 'distance_to_midpoint_km'] = Regression_Final1['distance_to_midpoint'] / 1000
#the scale should not dominate others numerically which is why the distance to the centre is being calculated in km

# Viewing the new column to ensure the operation worked
#Regression_Final1[['distance_to_midpoint', 'distance_to_midpoint_km']].head()
```

```{python}
# __________________ Running the regression model ____________


# Running the first regression model on the data
X = Regression_Final1[['total_airbnbs','VacantPercent','distance_to_midpoint_km']]  
y = Regression_Final1['Annual change']  


# Fitting the model
model = sm.OLS(y, sm.add_constant(X)).fit()

# Displaying model summary
print(model.summary())

# Plot residuals after the transformation
residuals = model.resid
fitted_values = model.fittedvalues

# ______________ Residuals vs Fitted plot
'''
plt.scatter(fitted_values, residuals)
plt.axhline(0, color='black', linestyle='--')
plt.title("Residuals vs Fitted Values")
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.show()
'''
# QQ plot
#sm.qqplot(residuals, line='45')
#plt.show()


# Breusch-Pagan test for heteroskedasticity
_, p_value, _, _ = sms.het_breuschpagan(residuals, model.model.exog)
#print(f'Breusch-Pagan p-value: {p_value}')

from statsmodels.stats.diagnostic import het_white

# White test for heteroskedasticity
_, p_value, _, _ = het_white(residuals, model.model.exog)
#print(f'White test p-value: {p_value}')
```


"The average rent in London is now £2,121 per month after +4.2% growth in the last year. The cheapest average rent is in Bexley (£1,520) and the highest average rent is in Kensington and Chelsea (£3,459), although rental increases are slowing in the most expensive parts of the city". (Zoopla, 2024) https://www.zoopla.co.uk/discover/property-news/average-rent-london/

**Interpreting the regression model**

Our analysis revealed that the rise in short-term rentals like Airbnb could play a key role in driving changes in rental prices. The data suggests that for every 1% increase in the number of Airbnb listings, the change in rental prices grew by 0.065%. In areas where Airbnb listings would have doubled (so an 100% increase), rents would have risen by an average of 6.5%. For tenants, this result in significant price increases: in central London, where average rents are around £2,121 per month, a 6.5% rise would add approximately £138 per month, or £1,656 per year. In high-end boroughs like Kensington and Chelsea, tenants would face rent hikes of nearly £2,700 per year. 

Besides Airbnb, the model results also show that proximity to the city center also would influence rental prices, with neighborhoods closer to the centre seeing an additional 0.34% for every kilometer closer. However, vacancy rates showed little connection to rent changes. These findings highlight how the expansion of Airbnb is reshaping London’s rental market, putting additional pressure on long-term tenants already facing rising costs.

However, the story doesn’t end with just the numbers. The analysis revealed that changes in rental prices are not evenly spread across the city. Although there were no signs of multicolinearity between the independent variables, meaning they didn’t overlap or interfere with each other. Tests on the errors (residuals) displayed uneven patterns in the data, and further spatial analysis confirmed significant clustering of rental price changes. Areas in central London stand out with clear High-High clusters, where rents are rising quickly and are surrounded by similarly high increases. In contrast, Low-Low clusters dominate the outer boroughs, where rent changes remain much smaller.

A closer look at the spatial patterns on the Local Morans I cluster map also uncovered outliers – places where neighborhoods with rising rents sit next to areas where prices are barely changing. These findings highlight the uneven impact of Airbnb and other pressures on London’s rental market, particularly in central areas.

To build on this analysis, future work could explore how neighborhoods influence each other by using tools that better capture these spatial relationships, including Spatial Lag or Spatial Error. Adding more information, including income levels and housing availability, could also help explain why rent changes happen in some places but not others. Additionaly, a more robust regression model can be undertaken that accounts for the uneven spread of residuals. 

```{python}
#checking the VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Add constant to the independent variables for VIF calculation
X_with_const = add_constant(X)

# Calculate VIF for each independent variable
vif_data = pd.DataFrame()
vif_data["Variable"] = X_with_const.columns
vif_data["VIF"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]

# Display VIF values
print(vif_data)

#No signs of multicolinearity 
```

**plotting the residuals**

```{python}
# ____________ checking morans I for any spatial autocorellation ____
from pysal.lib import weights

print(type(Regression_Final1))

# needs to be a geopandas data frame 
Rgdf = gpd.GeoDataFrame(Regression_Final1, geometry='geometry') #Rgdf - residuals gdf
print(type(Rgdf))
# Adding residuals to the GeoDataFrame
Rgdf['residuals'] = residuals

print(Rgdf.crs)

# ___________ Morans I ____________

# Generate spatial weights matrix
w = weights.Queen.from_dataframe(Rgdf)

# Standardize weights matrix (required for Moran's I)
w.transform = 'R'


mi = esda.Moran(Rgdf['residuals'], w)
print(f"{mi.I:0.4f}")
print(f"{mi.p_sim:0.4f}")

#moran_scatterplot(mi)

# ________ local Morans I on the residuals ___________

lisa = esda.Moran_Local(Rgdf.residuals, w)
# Break observations into significant or not
Rgdf['sig'] = lisa.p_sim < 0.05
# Store the quadrant they belong to
Rgdf['quad'] = lisa.q
Rgdf[['LAName','residuals','sig','quad']].sample(5, random_state=42)

#print(Rgdf)

plot_local_autocorrelation(lisa, Rgdf, 'residuals')


```


```{python}
# ______________ A more robust model but can do echo = false because its just adding too much _______

import statsmodels.api as sm


# Assuming 'X' is the independent variable(s) and 'y' is the dependent variable
model = sm.OLS(y, X)  # Create the OLS model
robust_results = model.fit(cov_type='HC0')  # Fit with heteroskedasticity-robust standard errors

# Print the summary
print(robust_results.summary())

# __________ plotting the residuals vs fitted and qq plot to see if this worked __________________________
# Residuals
residuals = robust_results.resid

# Fitted values (predicted values)
fitted_values = robust_results.fittedvalues


# QQ plot
sm.qqplot(residuals, line='45', fit=True)
plt.title('QQ Plot of Residuals (Robust Model)')
plt.show()


#residuals vs fitted values
plt.scatter(fitted_values, residuals, alpha=0.7)
plt.axhline(0, color='red', linestyle='--', linewidth=1)
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted Values (Robust Model)')
plt.show()
```

## 1. Who collected the InsideAirbnb data?

::: {.duedate}

( 2 points; Answer due Week 7 )

:::

An inline citation example: As discussed on @insideairbnb, there are many...

A parenthetical citation example: There are many ways to research Airbnb [see, for example, @insideairbnb]... 

## 2. Why did they collect the InsideAirbnb data?

::: {.duedate}

( 4 points; Answer due Week 7 )

:::

```{python}
#| output: asis
print(f"One of way to embed output in the text looks like this: after cleaning, we were left with {df.shape[0]:,} rows of data.")
```

This way is also supposed to work (`{python} f"{df.shape[0]:,}" `) but I've found it less reliable.

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

## 3. How was the InsideAirbnb data collected?  

::: {.duedate}

( 5 points; Answer due Week 8 )

:::

## 4. How does the method of collection impact the completeness and/or accuracy of the InsideAirbnb data set's representation of the process it seeks to study, and what wider issues does this raise?

::: {.duedate}

( 11 points; Answer due Week 9 )

:::

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 

::: {.duedate}

( 18 points; Answer due {{< var assess.group-date >}} )

:::

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and Listing types suggest about the nature of Airbnb lets in London? 

::: {.duedate}

( 15 points; Answer due {{< var assess.group-date >}} )

:::

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

::: {.duedate}

( 45 points; Answer due {{< var assess.group-date >}} )

:::

## Sustainable Authorship Tools

Using the Terminal in Docker, you compile the Quarto report using `quarto render <group_submission_file>.qmd`.

Your QMD file should automatically download your BibTeX and CLS files and any other required files. If this is done right after library loading then the entire report should output successfully.

Written in Markdown and generated from [Quarto](https://quarto.org/). Fonts used: [Spectral](https://fonts.google.com/specimen/Spectral) (mainfont), [Roboto](https://fonts.google.com/specimen/Roboto) (<span style="font-family:Sans-Serif;">sansfont</span>) and [JetBrains Mono](https://fonts.google.com/specimen/JetBrains%20Mono) (`monofont`). 

## References

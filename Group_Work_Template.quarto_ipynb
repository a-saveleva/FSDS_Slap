{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "bibliography: bio.bib\n",
        "csl: harvard-cite-them-right.csl\n",
        "title: Slap Group Project\n",
        "execute:\n",
        "  kernel: python3\n",
        "format:\n",
        "  html:\n",
        "    code-copy: true\n",
        "    code-link: true\n",
        "    toc: true\n",
        "    toc-title: On this page\n",
        "    toc-depth: 3\n",
        "    toc_float:\n",
        "      collapsed: false\n",
        "      smooth_scroll: true\n",
        "  pdf:\n",
        "    include-in-header:\n",
        "      text: |\n",
        "        \\addtokomafont{disposition}{\\rmfamily}\n",
        "    mainfont: Spectral\n",
        "    sansfont: Roboto Flex\n",
        "    monofont: InputMonoCondensed\n",
        "    papersize: a4\n",
        "    geometry:\n",
        "      - top=25mm\n",
        "      - left=40mm\n",
        "      - right=30mm\n",
        "      - bottom=25mm\n",
        "      - heightrounded\n",
        "    toc: false\n",
        "    number-sections: false\n",
        "    colorlinks: true\n",
        "    highlight-style: github\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    text_representation:\n",
        "      extension: .qmd\n",
        "      format_name: quarto\n",
        "      format_version: '1.0'\n",
        "      jupytext_version: 1.16.4\n",
        "  kernelspec:\n",
        "    display_name: Python (base)\n",
        "    language: python\n",
        "    name: base\n",
        "---"
      ],
      "id": "ae88a386"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl \n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from scipy.cluster.hierarchy import fcluster, linkage\n",
        "from scipy.stats import entropy\n",
        "\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from shapely.geometry import Point\n",
        "from urllib.parse import urlparse\n",
        "import base64  # Add this import at the top\n",
        "import IPython.display as display\n",
        "import random\n",
        "from pyproj import Proj, Transformer #for the distance to midpoint\n",
        "from scipy.stats import shapiro\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.stats.api as sms\n",
        "from statsmodels.stats.api import het_breuschpagan\n",
        "import esda\n",
        "from splot.esda import moran_scatterplot\n",
        "from splot.esda import plot_local_autocorrelation\n",
        "from scipy.stats import skew\n",
        "from libpysal.weights import Queen\n",
        "from libpysal import weights\n",
        "from shapely.ops import unary_union\n",
        "import contextily as ctx\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.options.display.max_colwidth = 200"
      ],
      "id": "2261efd4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "def cache_data(src:str) -> str:\n",
        "    \"\"\"Downloads and caches a remote file locally.\n",
        "    src : str - The remote *source* for the file, any valid URL should work.\n",
        "    dest : str - The *destination* location to save the downloaded file.    \n",
        "    Returns a string representing the local location of the file.\n",
        "    \"\"\"\n",
        "    dest = os.path.join('data','raw') #destination to save data\n",
        "    \n",
        "    url = urlparse(src) # We assume that this is some kind of valid URL \n",
        "    fn = (url.path.split('/')[-3])+(url.path.split('/')[-1]) # return the file name with date\n",
        "    dfn = os.path.join(dest,fn) #Â Destination filename\n",
        "    print(f\"Writing to: {fn}\")\n",
        "\n",
        "    if not os.path.isfile(dfn) or os.stat(dfn).st_size < 250:\n",
        "        \n",
        "        print(f\"{dfn} not found or corrupted, downloading!\")\n",
        "        path = os.path.split(dest)\n",
        "        if len(path) >= 1 and path[0] != '':\n",
        "            os.makedirs(os.path.join(*path), exist_ok=True)\n",
        "            # Download and save the file #os.path.join(*path)\n",
        "            try:\n",
        "                with open(dfn, \"wb\") as file:\n",
        "                    response = requests.get(src)\n",
        "                    response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "                    file.write(response.content)\n",
        "                print(\"\\tDone downloading.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                \n",
        "        f_size = os.stat(dfn).st_size\n",
        "        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n",
        "    else:\n",
        "        print(f\"Found {dfn} locally!\")\n",
        "        f_size = os.stat(dfn).st_size\n",
        "        print(f\"\\tSize is {f_size/1024**2:,.0f} MB ({f_size:,} bytes)\")\n",
        "        \n",
        "    return dfn"
      ],
      "id": "0c38955d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Set download URL for London data the date should be choosen by reviewing \n",
        "# the InsideAirbnb Get Data page and identifying the date of the required data\n",
        "# We are using the 2 datasets with earliest and latest scrape of those available\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "airbnbdates  = [\"2023-12-10\", \"2024-09-06\"]\n",
        "airbnbcols = ['id', 'listing_url', 'picture_url', 'host_id', 'host_listings_count',\n",
        "              'host_total_listings_count', 'property_type', 'room_type', 'price', \n",
        "              'minimum_nights', 'maximum_nights', 'availability_365', \n",
        "              'number_of_reviews', 'latitude', 'longitude', 'last_review', \n",
        "              'beds', 'bedrooms', 'host_location']\n",
        "airbnbdfs = []\n",
        "for date in airbnbdates:\n",
        "    airbnburl  = f\"https://data.insideairbnb.com/united-kingdom/england/london/{date}/data/listings.csv.gz\"\n",
        "    dfn = cache_data(airbnburl)\n",
        "    airbnbdfs.append(pd.read_csv(dfn, low_memory=False,usecols=airbnbcols))\n",
        "   \n",
        "airbnb_dflist = [airbnb_old, airbnb_new] = airbnbdfs #airbnb_old is the earlier scrape, and new is the later scrape. We don't hardcode the dates for reproducibility with other dates and times.\n",
        "\n",
        "airbnb_gdfs = []\n",
        "for df in airbnb_dflist:\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   I convert the df into gdf, reproject into EPSG 27700\n",
        "#_____________________________________________________________________________________________________________________________  \n",
        "    df = gpd.GeoDataFrame(\n",
        "        df,\n",
        "        geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
        "        crs='EPSG:4326')\n",
        "    \n",
        "    df = df.set_crs('EPSG:4326', allow_override=True)\n",
        "    df.to_crs('EPSG:27700', inplace=True)\n",
        "    airbnb_gdfs.append(df)"
      ],
      "id": "cdbcb8da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# InsideAirbnb data cleaning. We first define the functions to be used in cleaning.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "def pick_active_listings(df, date='str', col=\"str\"):\n",
        "    \"\"\"\n",
        "    Converting review column to datetime, and selecting only the recently active listings. \n",
        "    Recently active is defined as those which received at least one review for the year precedeng the scrape.\n",
        "    Arguments:\n",
        "    date: string, format \"%Y-%m-%d\"\n",
        "    dataframe\n",
        "    col: string, colname with \"last_review\" or its alternative\n",
        "    \"\"\"\n",
        "    df.loc[:, col] = pd.to_datetime(df[col])\n",
        "    date = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "    df = df[df['last_review'] > (date - relativedelta(years=1))] #| (df['last_review'].isnull())]\n",
        "    \n",
        "    print(f\"Data frame after filtering last review date is {df.shape[0]:,} x {df.shape[1]} long.\")\n",
        "    return df"
      ],
      "id": "755bb0f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Why do we remove duplicates?\n",
        "# In the regression part of our analysis, we  want to calculate N of homes that are somehow involved in Airbnb listings, and therefore don't fulle realise it's potential on traditional rental/housing market.\n",
        "# If we took just listing locations, with the amount of duplicates that we found, the N would show ACTIVITY of hosts more than loss of homes for housing market.\n",
        "# ChatGPT was used here to determine clustering methods, and for debugging.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "\n",
        "def find_duplicates(df_valid):\n",
        "    \"\"\"\n",
        "    The function looks at hosts with multiple listings and checks if they are within 300m radius (150m max location scattering as per Airbnb's anonymisation algorithm, x2).\n",
        "    It then estimates the number of genuine homes within the cluster.\n",
        "    \n",
        "    Returns a gdf with new columns: \n",
        "    'cluster_id' - each unique value is a cluster\n",
        "    'easting', 'northing' - used in calculating the proximity of listings\n",
        "    'prvt_rms_in_cluster','entr_hms_in_cluster' - summarises N of private rooms and entire homes within the cluster\n",
        "    'genuine', 'true_nrooms' - boolean column, and int column, with the following assumptions:\n",
        "\n",
        "    If the cluster has 0 entire homes, and N rooms, first room will be \"converted\" to an entire home, and the rest will be discarded. \n",
        "    The location of the point will be amended to represent centrepoint of all rooms in the cluster.\n",
        "    Record of the N of rooms will stay in col \"true_nrooms\"\n",
        "    If the cluster has rooms and entire homes, the homes will be treated as genuine homes, and rooms treated as duplicates of the genuine home. \n",
        "    With the current timeframes, further enquiries into uniqueness of homes vs rooms inside each cluster is not feasible.\n",
        "    \"\"\"\n",
        "    \n",
        "    df['easting'], df['northing'] = df.geometry.x, df.geometry.y #this is needed for the clustering  \n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   We are looking for duplicates among multi-listing hosts. First, we filter by new_listings_count > 1.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "    finddups = df_valid[df_valid['new_host_listings_count'] > 1]\n",
        "    print(f\"There are {finddups.shape[0]:,} listings from multi-listing hosts.\")\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   I perform cluster analysis a1nd mark listings from the same host that are within 300m from each other \n",
        "#   (150m max location scattering as per Airbnb's anonymisation algorithm, x2). They constitute a cluster, and we then assess if they are a duplicate or not.\n",
        "#   The selected method computes pairwise distances that can calculate the distance matrix between all points in a group. \n",
        "#   Before this method I tried KNN, but the listings were paired incorrectly. \n",
        "#   cdist solved this, and it is relatively light on small groups (our dataset is large, but it is split into small groups - by host).\n",
        "#\n",
        "#   It is possible that listings outside clusters also could be in one entire home. E.g. there could be several hosts registered for one home.\n",
        "#   We acknowledge this gap, and advise this for further research.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "    # Process each host group\n",
        "    for host_id, group in finddups.groupby(\"host_id\"):\n",
        "        if len(group) > 1:  # Only proceed if there is more than 1 listing\n",
        "            coords = group[['easting', 'northing']].to_numpy()\n",
        "            \n",
        "            # Calculate pairwise distances using pdist (returns condensed distance matrix)\n",
        "            dist_matrix = pdist(coords)\n",
        "            \n",
        "            # Perform hierarchical clustering using the condensed distance matrix\n",
        "            linkage_matrix = linkage(dist_matrix, method='single')\n",
        "            clusters = fcluster(linkage_matrix, t=300, criterion='distance')\n",
        "            \n",
        "            # Create a series to count cluster sizes\n",
        "            cluster_sizes = pd.Series(clusters).value_counts()\n",
        "    \n",
        "            # Assign clusters to rows in the group, but only for clusters with more than one member - these will retain default column value of None\n",
        "            finddups.loc[group.index, 'cluster_id'] = [\n",
        "                f\"{host_id}-{cluster}\" if cluster_sizes[cluster] > 1 else None\n",
        "                for cluster in clusters\n",
        "            ]\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   We create a subset with non-unique listings only to calculate true_nrooms and determine genuineness.\n",
        "#   At the very end of the function, this subset is merged back into df_valid.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "    finddups1 = finddups[~finddups['cluster_id'].isna()]\n",
        "    finddups1 = finddups1.loc[finddups.duplicated(subset='cluster_id', keep=False)]   \n",
        "    print(f\"Found {len(set(finddups1.cluster_id)):,} clusters.\")   \n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   I calculate N of entire homes and rooms in each cluster.\n",
        "#   Limitation found during EDA: \"bedrooms\" column in InsideAirbnb differs between scrapes. \n",
        "#   \"2023-12-10\" scrape has NaN values in this column, while \"2024-09-06\" scrape has full data.\n",
        "#   For both datasets, we use \"beds\" column as a substitute, because airbnb2024.bedrooms.median()/airbnb2024.beds.median() = 1.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "    finddups1 = finddups.loc[finddups.duplicated(subset='cluster_id',keep=False)]\n",
        "#   Loop through each cluster by cluster_id. \n",
        "    for i, group in finddups1.groupby(\"cluster_id\"):\n",
        "        # Count number of entire homes and private rooms in the group\n",
        "        n_entire_homes = group[group['room_type'] == 'Entire home/apt'].shape[0]\n",
        "        n_private_rooms = group[group['room_type'] == 'Private room'].shape[0]   \n",
        "        # Assign these counts back to the original DataFrame\n",
        "        finddups1.loc[group.index, 'entr_hms_in_cluster'] = n_entire_homes\n",
        "        finddups1.loc[group.index, 'prvt_rms_in_cluster'] = n_private_rooms          \n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   I then determine whether listings are genuine unique homes, or rooms from one entire home.\n",
        "#   If the cluster has 0 entire homes, and N rooms, first room will be \"converted\" to an entire home, and the rest will be discarded.\n",
        "#   Record of the N of rooms will stay in col \"true_nrooms\".\n",
        "#   If the cluster has rooms and entire homes, the homes will be treated as genuine homes, and rooms treated as duplicates of the genuine home.\n",
        "#   It is possible that all entire homes in one cluster are absolutely identical, but with the methods employed we can go this far. We acknowledge the gap and suggest it for further research.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "    for cluster_id, group in finddups1.groupby(\"cluster_id\"):\n",
        "        \n",
        "        num_entire_homes = (group['room_type'] == 'Entire home/apt').sum()\n",
        "        if num_entire_homes == 0:\n",
        "            # If only rooms are in the cluster\n",
        "            finddups1.loc[group.index, 'genuine'] = False  # Mark all as duplicates\n",
        "            finddups1.loc[group.index[0], 'genuine'] = True  # First room becomes the genuine home\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#           For remaining valid homes the geometry is replaced with centroid x and y, taken from points in cluster.  \n",
        "#           Without this the point and all associated rooms can be aggregated by different spatial unit, considering the scale of the location approximation (up to 150m from origin).\n",
        "#           This is not crucial for our research, because we aggregate by borough, and potential aggregation errors will be minimal. But for other research with smaller geospatial units it will prove more useful.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "            dissolved_geometry = group.geometry.union_all()  # Combine all geometries in the group\n",
        "            centroid = dissolved_geometry.centroid # Get the centroid of the combined geometry\n",
        "            finddups1.at[group.index[0], 'geometry'] = Point(centroid.x, centroid.y)\n",
        "        else:\n",
        "            # Cluster has both rooms and entire homes\n",
        "            finddups1.loc[group.index, 'genuine'] = False  # Default all to duplicates\n",
        "            entire_home_indices = group[group['room_type'] == 'Entire home/apt'].index\n",
        "            finddups1.loc[entire_home_indices, 'genuine'] = True  # Mark entire homes with true\n",
        "\n",
        "    print(f\"The clustering exercise identified {len(finddups1[finddups1['genuine'] == False])} listings that are potentially duplicates of other listings.\")\n",
        "    df_valid.update(finddups1)\n",
        "    \n",
        "    return df_valid"
      ],
      "id": "3ff4e394",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "airbnb_gdf_clean = []\n",
        "airbnb_gdf_ingenuine = []\n",
        "airbnb_gdf_all = []\n",
        "\n",
        "for df in airbnb_gdfs:\n",
        "    print(f\"Initial dataframe is {df.shape[0]:,} x {df.shape[1]} long.\")\n",
        "    df['price'] = df['price'].fillna('0').str.replace('$', '', regex=False).str.replace(',', '').astype(float).astype(int)\n",
        "    df.drop(df[df[\"price\"]==0].index, axis=0, inplace=True)\n",
        "    print(f\"Data frame after filtering invalid prices is {df.shape[0]:,} x {df.shape[1]} long.\")\n",
        "    \n",
        "    df = pick_active_listings(df, date, \"last_review\")\n",
        "\n",
        "    ints  = ['id', 'host_listings_count', 'host_id', 'bedrooms', 'beds', \n",
        "             'host_total_listings_count', 'minimum_nights', 'maximum_nights', 'availability_365', 'number_of_reviews']\n",
        "    for i in ints:\n",
        "        #print(f\"Converting {i}\")\n",
        "        try:\n",
        "            df.loc[:, i] = df[i].fillna(0).astype(float).astype(int)\n",
        "        except ValueError as e:\n",
        "            #print(\"  - !!!Converting to unsigned 16-bit integer!!!\")\n",
        "            df.loc[:, i] = df[i].fillna(0).astype(float).astype(pd.UInt16Dtype())\n",
        "    #_____________________________________________________________________________________________________________________________\n",
        "    # After these transformations, the N of listings for each host has changed. We dropped rows where listings were likely inactive at the time of the scrape.\n",
        "    # We create a new column with host_listings_count, and calculate the counts using grouping by host_id.\n",
        "    #_____________________________________________________________________________________________________________________________\n",
        "    host_counts = df['host_id'].value_counts()\n",
        "    df = pd.merge(df,host_counts,right_index=True,left_on='host_id').rename(columns={'count':'new_host_listings_count'})\n",
        "        \n",
        "    # null values in N of bedrooms will interfere with our analysis, therefore we drop them\n",
        "    df = df[df['beds'].notna()]\n",
        "    print(f\"Data frame after cleaning invalid N of beds is {df.shape[0]:,} listings long.\")\n",
        "    \n",
        "    # creating columns necessary for the analysis and filling with default values\n",
        "    df['prvt_rms_in_cluster'] = df['beds']* 0.8 #be are using beds for rooms, but for now just copy the column\n",
        "    # airbnb_gdfs[1][\"bedrooms\"].mean()/airbnb_gdfs[1][\"beds\"].mean() = 0.8\n",
        "    df['entr_hms_in_cluster'] = 0 #default will be 0\n",
        "    df['cluster_id'] = None #will write ids here\n",
        "    #df['true_nrooms'] = df['beds'] #estimation of true n of rooms proved inconclusive with the methods used. The parts related to rooms are commented out.\n",
        "    df['genuine'] = True   \n",
        "    \n",
        "    dups = find_duplicates(df) \n",
        "    airbnb_gdf_all.append(dups)\n",
        "    airbnb_gdf_ingenuine.append((dups[dups['genuine'] == False]).copy()) #collecting False for validation\n",
        "    \n",
        "    df = (df[df['genuine'] == True]).copy()\n",
        "    df = df[(df['availability_365']) > (5.84*12)] #Filtering by \"commercial\" - rented out more than average per month for London. Average is 5.84 days\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#https://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/guestnightsnightsandstaysforshorttermletsuk\n",
        "#https://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/hostslistingsandbedspacesofshorttermletsuk2023\n",
        "#According to this data, in December 2023 (month where there is overlap between 2 datasets), there were 152,050 nights spent in all 111,880 observed STLs in London, \n",
        "#5.84 nights per listing. We take this as a benchmark for \"fair\" use of Airbnb in London. Hosts that are available for longer than 5.84*12 days will be considereded \n",
        "#as those taking advantage of the opportunity, which also means there is a possibility that they don't live in the flat - we will chech this later.\n",
        "# due to limitations, we can't see the actual occupancy per listing, and have to treat the availability as occupancy. \n",
        "# assuming that if the host was non-commercial, they would block out more days when they are living in the flat, and the availability will be lower.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "\n",
        "    print(f\"Data frame after cleaning duplicate listings is {df.shape[0]:,} listings long (multi-listing hosts only).\\n\")\n",
        "    print(f\"Appending cleaned data frame to single-listing hosts data frame, and storing everything in list airbnb_gdf_clean.\")\n",
        "    \n",
        "    airbnb_gdf_clean.append(df)\n",
        "    print(\"Done.\\n\")"
      ],
      "id": "8ce44a08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "airbnb_gdf_clean[0].head(3) #this prints 1st element of the list, so the older dataset\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# At this point, airbnb_gdf_clean is a list, which contains [0] - clean dataset of older airbnb data, and [1] - clean newer data\n",
        "#_____________________________________________________________________________________________________________________________"
      ],
      "id": "f1fd9611",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#   You might wonder, whether the find_duplicates function works properly.\n",
        "#   We acknowledge that, given the timeframes, and considering that our research question is not specifically about duplicates,\n",
        "#   it is not feasible to develop a complex validation mechanism.\n",
        "#   However, you can see for yourself the results of the clustering and our assumptions, using the function below.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "\n",
        "# This was not developed as a proper validation tool, only as a proof of concept of finding duplicates.\n",
        "# As this is not the main part of the exercise, ChatGPT was heavily used here to set up thumbnail plotting.\n",
        "\n",
        "def check_genuine(df, **kwargs):\n",
        "    \"\"\"\n",
        "    Receives df and random_state as args. If unspecified, gives random number as random_state.\n",
        "    Displays the random ingenuine listing, and other listings from its cluster as clickable image thumbnails.\n",
        "    \"\"\"\n",
        "    if not kwargs.get('random_state'):\n",
        "        random_state = random.randint(0,1000)\n",
        "    else:\n",
        "        random_state = kwargs.get('random_state')\n",
        "    print(f'Random_state unspecified, selecting:{random_state}.')\n",
        "    \n",
        "    # Select a random \"ingenuine\" listing from the dataset\n",
        "    listing_check = df[df['genuine'] == False].sample(1, random_state=random_state)\n",
        "    \n",
        "    print(f\"Let's have a look at the random listing. ID: {listing_check.id.values[0]}\")\n",
        "    print(f\"Cluster number {listing_check.cluster_id.values[0]}\")\n",
        "    \n",
        "    if (listing_check['beds'] > 4).any():\n",
        "        print(\"Our estimate is that this might be a hostel, party home or similar hishly commercialised listing.\")\n",
        "    else:\n",
        "        print(\"Our estimate is that this cluster might be one entire home.\\n\")\n",
        "    \n",
        "    print(\"All listings in this cluster are below. Click on the image to open the listings on website.\")\n",
        "    print(\"You might get inactive links with past datasets.\")\n",
        "    \n",
        "    # Get all listings in the same cluster\n",
        "    same_cluster = df[df[\"cluster_id\"] == listing_check.cluster_id.values[0]]\n",
        "    # print(same_cluster[[\"id\", \"listing_url\"]])\n",
        "    \n",
        "    # Collect successful images and their associated URLs\n",
        "    successful_images = []\n",
        "    urls = []\n",
        "    \n",
        "    for idx, row in same_cluster.iterrows():\n",
        "        try:\n",
        "            response = requests.get(row['picture_url'], timeout=10)\n",
        "            response.raise_for_status()  # Raise an error for HTTP issues\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            img.thumbnail((190, 190))  # Resize image for thumbnail display\n",
        "            successful_images.append((img, row['listing_url']))\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to retrieve image for Listing ID: {row['id']}. Error: {e}\")\n",
        "    \n",
        "    # If no successful images, return early\n",
        "    if not successful_images:\n",
        "        print(\"No images could be retrieved.\")\n",
        "        return\n",
        "    \n",
        "    # Render images and clickable links in Quarto Markdown\n",
        "    html_content = \"<table>\"\n",
        "    for i, (img, url) in enumerate(successful_images):\n",
        "        if i % 3 == 0:  # Start a new row every 3 images\n",
        "            html_content += \"<tr>\"\n",
        "        \n",
        "        # Convert the image to a data URL\n",
        "        img_buffer = BytesIO()\n",
        "        img.save(img_buffer, format=\"PNG\")\n",
        "        img_data = img_buffer.getvalue()\n",
        "        img_base64 = base64.b64encode(img_data).decode('utf-8')  # Encoding the image\n",
        "        img_src = f\"data:image/png;base64,{img_base64}\"\n",
        "        \n",
        "        # Add the image and link to the HTML\n",
        "        html_content += f\"\"\"\n",
        "        <td style=\"text-align:center; padding:5px\">\n",
        "            <a href=\"{url}\" target=\"_blank\"><img src=\"{img_src}\" style=\"width:128px; height:auto; border:1px solid #ccc\" /></a>\n",
        "        </td>\n",
        "        \"\"\"\n",
        "        if i % 3 == 2:  # End the row every 3 images\n",
        "            html_content += \"</tr>\"\n",
        "    \n",
        "    html_content += \"</table>\"\n",
        "    \n",
        "    # Display the HTML in Quarto\n",
        "    display.display(display.HTML(html_content))"
      ],
      "id": "b350163e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "check_genuine(airbnb_gdf_all[1]) #you can also define random state, e.g. check_genuine(airbnb_gdf_all[1],random_state=10)\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# The algorithm shows 67-74% accuracy, validated with randomised sampling of 25 clusters. See validation sheet in \"notebooks\", \"Cluster validation sheet\".\n",
        "#_____________________________________________________________________________________________________________________________"
      ],
      "id": "e06be0a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Loading other data - Csv\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "raw_csvs_fn = ['Census_LSOA_Dwellings', 'Census_dwellings' \n",
        "    ,'Census_Households', 'Census_Tenure', 'Census_Accommodation', \n",
        "    'Census_BedroomOcc', 'Census_CarsVans', 'Census_Deprivation', 'Census_NsSeC', \n",
        "    'Census_PopDensity', 'Census_Addresses', 'Sales-2024-09', \n",
        "    'First-Time-Buyer-Former-Owner-Occupied-2024-09', 'New-and-Old-2024-09', 'onsrents']\n",
        "csv_skiprows = [6, 6 ,6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, None]\n",
        "csv_nrows = [4994, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 138543, 60219, 138543, 38390]\n",
        "csv_parse_dates = [None, None, None, None, None, None, None, None, None, None, None, None, ['Date'], ['Date'], ['Time period']]\n",
        "csv_usecols = [None, \n",
        "    ['local authority: county / unitary (as of April 2023)', 'mnemonic', 'Total'] \n",
        "    ,['mnemonic', '2021'],  \n",
        "    ['mnemonic', 'Owned', 'Shared ownership', 'Social rented', 'Private rented or lives rent free'], \n",
        "    ['mnemonic', 'Detached', 'Semi-detached', 'Terraced', 'In a purpose-built block of flats or tenement','Part of a converted or shared house, including bedsits', 'Part of another converted building, for example, former school, church or warehouse', 'In a commercial building, for example, in an office building, hotel or over a shop'], \n",
        "    ['mnemonic', 'Occupancy rating of bedrooms: +2 or more', 'Occupancy rating of bedrooms: +1', 'Occupancy rating of bedrooms: 0', 'Occupancy rating of bedrooms: -1', 'Occupancy rating of bedrooms: -2 or less'],\n",
        "    ['mnemonic', 'No cars or vans in household', '1 car or van in household', '2 cars or vans in household', '3 or more cars or vans in household'], \n",
        "    ['mnemonic', 'Household is not deprived in any dimension', 'Household is deprived in one dimension', 'Household is deprived in two dimensions', 'Household is deprived in three dimensions', 'Household is deprived in four dimensions'],\n",
        "    ['mnemonic', 'L1, L2 and L3 Higher managerial, administrative and professional occupations', 'L4, L5 and L6 Lower managerial, administrative and professional occupations', \n",
        "    'L7 Intermediate occupations', 'L8 and L9 Small employers and own account workers', 'L10 and L11 Lower supervisory and technical occupations', 'L12 Semi-routine occupations', \n",
        "    'L13 Routine occupations', 'L14.1 and L14.2 Never worked and long-term unemployed', 'L15 Full-time students'], \n",
        "    ['mnemonic', '2021'], \n",
        "    ['mnemonic', 'Address one year ago is the same as the address of enumeration', 'Address one year ago is student term-time or boarding school address in the UK', 'Migrant from within the UK: Address one year ago was in the UK', 'Migrant from outside the UK: Address one year ago was outside the UK'],\n",
        "    None,\n",
        "    ['Date', 'Area_Code', 'First_Time_Buyer_Average_Price', 'Former_Owner_Occupier_Average_Price'],\n",
        "    ['Date', 'Area_Code', 'New_Build_Average_Price', 'Existing_Property_Average_Price'], \n",
        "    None]\n",
        "\n",
        "csv_dict = {\n",
        "    \"raw_csvs_fn\": raw_csvs_fn,\n",
        "    \"csv_skiprows\": csv_skiprows,\n",
        "    \"csv_nrows\": csv_nrows,\n",
        "    \"csv_parse_dates\": csv_parse_dates,\n",
        "    \"csv_usecols\": csv_usecols,\n",
        "}\n",
        "data_df = []\n",
        "for idx, item in enumerate(csv_dict[\"raw_csvs_fn\"]):\n",
        "    df = pd.read_csv(f\"data/raw/{item}.csv\", \n",
        "         skiprows = csv_dict[\"csv_skiprows\"][idx], \n",
        "         nrows = csv_dict[\"csv_nrows\"][idx], \n",
        "         usecols = csv_dict[\"csv_usecols\"][idx] \n",
        "         ,parse_dates = csv_dict[\"csv_parse_dates\"][idx]\n",
        "    )\n",
        "    data_df.append(df)"
      ],
      "id": "e1ef014f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Loading other data - Excel\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "raw_excels_fn = [\n",
        "    'vacant-dwellings-borough.xlsx','hpssadataset9medianpricepaidforadministrativegeographies.xls',\n",
        "    'hpssadataset15lowerquartilepricepaidforadministrativegeographies.xls',\n",
        "    'Detailed_LA_20232024.xlsx','privaterentalmarketstatistics231220.xls','privaterentalmarketstatistics231220.xls',\n",
        "    'privaterentalmarketstatistics231220.xls','privaterentalmarketstatistics231220.xls'  \n",
        "]\n",
        "excel_skiprows = [1, 6, 6, 7, 6, 6, 6, 6]\n",
        "excel_nrows = [34, 331, 331, 310, 357, 357, 357, 357]\n",
        "excel_sheet_name = [ 2, 10, 10, 3, 12, 13, 14, 15]\n",
        "excel_usecols = [\n",
        "[0,21], [2,113], [2,113],[0,4,6,16],['Area Code1', 'Mean', 'Median'],\n",
        "['Area Code1', 'Mean', 'Median'],['Area Code1', 'Mean', 'Median'],['Area Code1', 'Mean', 'Median']\n",
        "]\n",
        "excel_dict = {\n",
        "    \"raw_excels_fn\": raw_excels_fn,\n",
        "    \"excel_skiprows\": excel_skiprows,\n",
        "    \"excel_nrows\": excel_nrows,\n",
        "    \"excel_sheet_name\": excel_sheet_name,\n",
        "    \"excel_usecols\": excel_usecols,\n",
        "}\n",
        "for idx, item in enumerate(excel_dict[\"raw_excels_fn\"]):\n",
        "    df = pd.read_excel(os.path.join('data','raw', f\"{item}\"), \n",
        "         skiprows = excel_dict[\"excel_skiprows\"][idx], \n",
        "         nrows = excel_dict[\"excel_nrows\"][idx], \n",
        "         usecols = excel_dict[\"excel_usecols\"][idx] \n",
        "         ,sheet_name = excel_dict[\"excel_sheet_name\"][idx]\n",
        "    )\n",
        "    first_col_name = df.columns[0]\n",
        "    df.rename(columns={first_col_name: 'mnemonic'}, inplace=True)\n",
        "    \n",
        "    data_df.append(df)"
      ],
      "id": "b66e940c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Drop NA and non-London rows. Filter the last step, because first item has codes by LSOA, not by borough\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "for idx, df in enumerate(data_df):\n",
        "    for col in df.columns:\n",
        "        if isinstance(col, str) and pd.Series(col).str.contains('area.*code', case=False).any():\n",
        "            df.rename(columns={col: 'mnemonic'}, inplace=True)  \n",
        "    data_df[idx] = df[df['mnemonic'].notna()]\n",
        "\n",
        "for idx, df in enumerate(data_df[1:], start=1):\n",
        "    data_df[idx] = df[df['mnemonic'].str.startswith('E09', na=False)]"
      ],
      "id": "8853ff6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "(df_censusLSOAdwell, df_censusdwellings, df_censusHHolds, df_censusTenure, df_censusaccomm, \n",
        "df_censusbedocc, df_censuscarsvans, df_censusdepriv, df_censusnssec, df_censuspopdens, df_censusaddress, df_hp_sales, df_hp_firsttime, \n",
        "df_hp_newbuild, df_rentalprice, df_vacantprops,df_hp_median,df_hp_lower, df_homless_A2P,df_PRS1bed,df_PRS2bed,df_PRS3bed,df_PRS4bed\n",
        ") = data_df #We unpack the list, and assign new variable names"
      ],
      "id": "a380321a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Calculate entropy\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "for_entropy = [df_censusTenure, df_censusaccomm, df_censusbedocc, df_censuscarsvans, df_censusdepriv, df_censusnssec, df_censusaddress]\n",
        "for_entropy_strings = ['Tenure_entropy', 'Accommm_entropy', 'Bedocc_entropy', 'Cars_entropy', 'Depriv_entropy', 'NsSEC_entropy', 'Address_entropy']\n",
        "\n",
        "for idx, i in enumerate(for_entropy):\n",
        "    array = np.array(i.iloc[:, 1:])  # Select all rows and columns starting from the second column\n",
        "    entropy_values = np.apply_along_axis(entropy, axis=1, arr=array)  # Apply entropy along each row (axis=1)\n",
        "    i[f\"{for_entropy_strings[idx]}\"] = entropy_values"
      ],
      "id": "0a8f0210",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Rename columns for clarity\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "df_censusdwellings = df_censusdwellings.rename(\n",
        "    columns={'local authority: county / unitary (as of April 2023)': 'LAName',\n",
        "             'Total': 'Total_Dwellings'}\n",
        ")\n",
        "df_censusHHolds = df_censusHHolds.rename(columns={'2021': 'Total_Hholds'})\n",
        "df_censuspopdens = df_censuspopdens.rename(columns={'2021': 'PopDensity'})\n",
        "df_vacantprops = df_vacantprops.rename(columns={2023: '2023_LTVacant'})\n",
        "df_hp_median = df_hp_median.rename(columns={'Year ending Mar 2023': 'MedianHP_2023'})\n",
        "df_hp_lower = df_hp_lower.rename(columns={'Year ending Mar 2023': 'LowerQHP_2023'})\n",
        "\n",
        "df_homless_A2P = df_homless_A2P.rename(columns={146430: 'HHoldsPrevDuty', 57340: 'HHoldEndAST', 37040: 'HHoldSellRelet'})\n",
        "\n",
        "df_PRS1bed = df_PRS1bed.rename(columns={'Mean': 'MeanOneBedRent2023', 'Median': 'MedianOneBedRent2023'})\n",
        "df_PRS2bed = df_PRS2bed.rename(columns={'Mean': 'MeanTwoBedRent2023', 'Median': 'MedianTwoBedRent2023'})\n",
        "df_PRS3bed = df_PRS3bed.rename(columns={'Mean': 'MeanThreeBedRent2023', 'Median': 'MedianThreeBedRent2023'})\n",
        "df_PRS4bed = df_PRS4bed.rename(columns={'Mean': 'MeanFourPlusBedRent2023', 'Median': 'MedianFourPlusBedRent2023'})\n",
        "df_rentalprice = df_rentalprice.rename(columns={'Time period': 'Date'})"
      ],
      "id": "17bbfafb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Filter sales data for the finantial year\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "time_fltr = [df_hp_sales, df_hp_firsttime, df_hp_newbuild, df_rentalprice]\n",
        "for idx, i in enumerate(time_fltr):\n",
        "    i[\"Date\"] = pd.to_datetime(i[\"Date\"], errors='coerce')\n",
        "    time_fltr[idx] = i.query(\"Date >= '2023-04-01' and Date < '2024-03-31'\")\n",
        "df_hp_sales, df_hp_firsttime, df_hp_newbuild, df_rentalprice = time_fltr"
      ],
      "id": "88a39c11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Calculate totals\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "monthly_avg_rent = df_rentalprice.groupby(['mnemonic'])['Rental price'].mean().reset_index()\n",
        "df_hp_firsttime_totals = df_hp_firsttime.groupby(['mnemonic']).mean(['First_Time_Buyer_Average_Price', 'Former_Owner_Occupier_Average_Price'])\n",
        "df_hp_totals = df_hp_sales.groupby(['mnemonic']).sum(['Sales_Volume'])\n",
        "df_hp_newbuild_totals = df_hp_newbuild.groupby(['mnemonic']).mean(['New_Build_Average_Price', 'Existing_Property_Average_Price'])\n",
        "\n",
        "# df_homless_A2P.dropna(how='all', axis=1, inplace=True) \n",
        "# df_homless_A2P.dropna(how='all', axis=0, inplace=True) "
      ],
      "id": "0db74970",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Transfrom columns\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "df_homless_A2P['HHoldsPrevDuty'] = pd.to_numeric(df_homless_A2P['HHoldsPrevDuty'], errors='coerce')\n",
        "df_homless_A2P['HHoldEndAST'] = pd.to_numeric(df_homless_A2P['HHoldEndAST'], errors='coerce')\n",
        "df_homless_A2P['HHoldSellRelet'] = pd.to_numeric(df_homless_A2P['HHoldSellRelet'], errors='coerce')\n",
        "df_PRS3bed['MeanThreeBedRent2023'] = pd.to_numeric(df_PRS3bed['MeanThreeBedRent2023'], errors='coerce')\n",
        "df_PRS3bed['MedianThreeBedRent2023'] = pd.to_numeric(df_PRS3bed['MedianThreeBedRent2023'], errors='coerce')\n",
        "df_PRS4bed['MeanFourPlusBedRent2023'] = pd.to_numeric(df_PRS4bed['MeanFourPlusBedRent2023'], errors='coerce')\n",
        "df_PRS4bed['MedianFourPlusBedRent2023'] = pd.to_numeric(df_PRS4bed['MedianFourPlusBedRent2023'], errors='coerce')"
      ],
      "id": "b14a41c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Creating core datasets to be used in analysis: Housing data\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "housing_merge = [df_censusdwellings, df_censusHHolds, df_vacantprops, df_hp_median,\n",
        "                  df_hp_lower, df_hp_totals, df_homless_A2P, df_hp_firsttime_totals,\n",
        "                  df_hp_newbuild_totals, df_censusTenure, monthly_avg_rent]\n",
        "\n",
        "df_CoreHousing = housing_merge[0]\n",
        "for df in housing_merge[1:]:\n",
        "    df_CoreHousing = pd.merge(df_CoreHousing, df, on='mnemonic', how='outer')\n",
        "\n",
        "df_CoreHousing['PrevDutyPercent'] = ((df_CoreHousing['HHoldsPrevDuty']) / (df_CoreHousing['Total_Hholds']))*100\n",
        "df_CoreHousing['EndASTPercent'] = ((df_CoreHousing['HHoldEndAST']) / (df_CoreHousing['Total_Hholds']))*100\n",
        "df_CoreHousing['SellReletPercent'] = ((df_CoreHousing['HHoldSellRelet']) / (df_CoreHousing['Total_Hholds']))*100\n",
        "df_CoreHousing['VacantPercent'] = ((df_CoreHousing['2023_LTVacant']) / (df_CoreHousing['Total_Dwellings']))*100\n",
        "df_CoreHousing['SalesPercent'] = ((df_CoreHousing['Sales_Volume']) / (df_CoreHousing['Total_Dwellings']))*100\n",
        "#df_CoreHousing"
      ],
      "id": "9bcf4605",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#df_censusdwellings['Total_Dwellings'].sum()"
      ],
      "id": "e2f8ad5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Creating core datasets to be used in analysis: Demographics data\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "demographics_merge = [df_censusdwellings, df_censusHHolds, df_censusTenure, df_censuspopdens,\n",
        "                  df_censusaddress, df_censuscarsvans, df_censusdepriv, df_censusnssec,\n",
        "                  df_censusbedocc, df_censusaccomm]\n",
        "\n",
        "df_CoreDemographics = demographics_merge[0]\n",
        "for df in demographics_merge[1:]:\n",
        "    df_CoreDemographics = pd.merge(df_CoreHousing, df, on='mnemonic', how='outer')\n",
        "#df_CoreDemographics"
      ],
      "id": "94b63ff8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Perform some spatial data wrangling on the airbnb data then load spatial data (boroughs, lsoa) and join the airbnb data to the spatial data.\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "rents_merge = [df_PRS1bed, df_PRS2bed, df_PRS3bed, df_PRS4bed]\n",
        "df_PrivateRentsRooms = rents_merge[0]\n",
        "for df in rents_merge[1:]:\n",
        "    df_PrivateRentsRooms = pd.merge(df_PrivateRentsRooms, df, on='mnemonic', how='outer')\n",
        "#df_PrivateRentsRooms"
      ],
      "id": "4566c322",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Performing spatial joins of Airbnb data by borough\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "\n",
        "# Load London Borough and LSOA shapefiles - boroughs downloaded from \"https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip\"\n",
        "boro_shp = os.path.join(\"data\", \"london-boundaries\", \"statistical-gis-boundaries-london\", \"ESRI\", \"London_Borough_Excluding_MHW.shp\")  \n",
        "lsoa_shp = os.path.join(\"data\", \"lsoa-boundaries\", \"merged_lsoa_boundaries_london.shp\")  \n",
        "\n",
        "# read shapefile into gdf and reproject crs to 27700\n",
        "boro_gdf = gpd.read_file(boro_shp).to_crs('EPSG:27700')[['NAME', 'GSS_CODE', 'ONS_INNER', 'geometry']]\n",
        "lsoa_gdf = gpd.read_file(lsoa_shp).to_crs('EPSG:27700')[['lsoa21cd', 'lsoa21nm', 'lad22cd', 'lad22nm', 'geometry']]\n",
        "\n",
        "# quickly plot the data\n",
        "#boro_gdf.plot()\n",
        "#lsoa_gdf.plot()"
      ],
      "id": "e65feb88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "airbnb_gdf_clean[1].columns\n",
        "#len(airbnb_gdf_clean[1])"
      ],
      "id": "47569cd6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "def airbnb_count(gdf, boro_gdf):\n",
        "    \"\"\"\n",
        "    Function to count the airbnb locations within each borough and LSOA, split by room type.\n",
        "    Args:\n",
        "    gdf: airbnb gdf\n",
        "    boro_gdf: geographic areas\n",
        "    Both gdfs should be in the same projection.\n",
        "    \"\"\" \n",
        "    #clip by London boundary\n",
        "    gdf = gdf.clip(boro_gdf)\n",
        "    \n",
        "    # counts the airbnb locations within each borough, split by room type\n",
        "    # set new gdf for each room type\n",
        "    entire_ab_gdf = gdf[gdf['room_type'] == \"Entire home/apt\"]\n",
        "    pr_ab_gdf = gdf[gdf['room_type'] == \"Private room\"]\n",
        "    #other_ab_gdf = gdf[~gdf['room_type'].isin([\"Entire home/apt\", \"Private room\"])]\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "#Due to the relatively low counts of \"other\" room types we can discount these from our analysis and focus \n",
        "#on \"Entire Home/apts\" and \"Private rooms\"\n",
        "#_____________________________________________________________________________________________________________________________   \n",
        "    # create point counts per borough\n",
        "    c1 = boro_gdf.join(gpd.sjoin(gdf, boro_gdf).groupby(\"index_right\").size().rename(\"total_airbnbs\"))\n",
        "    c2 = c1.join(gpd.sjoin(entire_ab_gdf, c1).groupby(\"index_right\").size().rename(\"total_entireha\"))\n",
        "    boro_count_gdf = c2.join(gpd.sjoin(pr_ab_gdf, c2).groupby(\"index_right\").size().rename(\"total_privateroom\"))\n",
        "    #boro_count_gdf = c3.join(gpd.sjoin(other_ab_gdf, c3).groupby(\"index_right\").size().rename(\"total_other\"))\n",
        "    \n",
        "    # replaces NaN values with zero\n",
        "    ## could build funtion to iterate through this\n",
        "    boro_count_gdf['total_entireha'] = boro_count_gdf['total_entireha'].fillna(0).astype('int64')\n",
        "    boro_count_gdf['total_privateroom'] = boro_count_gdf['total_privateroom'].fillna(0).astype('int64')\n",
        "    #boro_count_gdf['total_other'] = boro_count_gdf['total_other'].fillna(0).astype('int64')\n",
        "\n",
        "    airbnb_count_boro = [boro_count_gdf, entire_ab_gdf, pr_ab_gdf\n",
        "                         #,other_ab_gdf\n",
        "                        ]\n",
        "    \n",
        "    return airbnb_count_boro"
      ],
      "id": "7c273aa4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Making the counts of airbnbs\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "airbnb_2024_boro = airbnb_count(airbnb_gdf_clean[1], boro_gdf) #list of [boro_count_gdf, entire_ab_gdf, pr_ab_gdf]\n",
        "airbnb_2023_boro = airbnb_count(airbnb_gdf_clean[0], boro_gdf)\n",
        "airbnb_2024_lsoa = airbnb_count(airbnb_gdf_clean[1], lsoa_gdf)\n",
        "airbnb_2023_lsoa = airbnb_count(airbnb_gdf_clean[0], lsoa_gdf)"
      ],
      "id": "7dd92a1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "len(airbnb_2024_boro[1]), len(airbnb_2023_boro[1]), len(airbnb_2024_lsoa[1]), len(airbnb_2023_lsoa[1])"
      ],
      "id": "494c2536",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "# Creating a basic map of airbnb locations by borough\n",
        "# lsoa is not being plotted as at this scale it will overcomplicate the map\n",
        "#_____________________________________________________________________________________________________________________________\n",
        "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
        "\n",
        "# Plot all borough and airbnb data to the same axes\n",
        "boro_gdf.plot(edgecolor='red', facecolor='none', ax=ax)\n",
        "airbnb_2024_boro[1].plot(edgecolor='blue', facecolor='none', ax=ax,alpha=0.25, markersize=1.0)\n",
        "airbnb_2024_boro[2].plot(edgecolor='green', facecolor='none', ax=ax, alpha=0.25, markersize=1.0)\n",
        "#airbnb_2024_boro[3].plot(edgecolor='orange', facecolor='none', ax=ax, alpha=0.25, markersize=1.0)\n",
        "\n",
        "# Set the x and y limits\n",
        "ax.set_xlim(501000, 563000)\n",
        "ax.set_ylim(155000, 202000)\n",
        "\n",
        "# Save the image (dpi is 'dots per inch')\n",
        "#os.makedirs('img', exist_ok=True)\n",
        "#plt.savefig(os.path.join('img','borough_map_with_air_bnb_locations.png'), dpi=150)"
      ],
      "id": "759ae267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Growing number of listings\n",
        "We identified 28,207 active London listings in 2024 - 55.22% year-on-year increase. The listings accounted for 0.48% and 0.74% of total dwellings respectively [@GovUK2012, @powerbiReport2024]; entire homes - 0.35% and 0.58% respectively, in contrast to other sources [@EY_Airbnb_2024]. The central areas exhibit the highest proportion of Airbnb listings relative to total dwellings, and this share increased progressively across these and adjacent boroughs from 2023 to 2024.\n",
        "### Trend towards entire homes\n",
        "In 2024, the percentage of entire homes on Airbnb increased to 78%, up from 73% in 2023. The share of private rooms decreased from 26% to 21%, indicating a trend towards more entire homes, which have a greater impact on housing. Some sources suggest STL ease and higher revenue may incentivize removing properties from long-term rental markets [@cromarty2024, @evans2024], even if they start STL as a private room. We therefore see the need to expand the methodology to include private rooms into the research scope.\n",
        "### More multi-listing hosts\n",
        "The proportion of multi-listing hosts increased from 53% in 2023 to 57% in 2024, with 36% of these hosts having two listings. The average number of listings per multi-listing host rose from 14.8 to 19.5, and nearly 10% now manage more than 10 listings. Notably, there is growing prevalence of one-bedroom and 3+ bedroom listings, suggesting that multi-listing hosts are increasingly occupying larger homes and one-bedroom properties. This trend may limit access to family-sized housing and properties for first buyers. The high average number of listings likely reflects the involvement of asset management companies, pointing to a trend towards greater commercialisation of the platform.\n",
        "We compare our observations with the Airbnb-endorsed studies [@EY_Airbnb_2024, @scanlon2024]. As seen from our EDA, the commercial use of the platform is increasing, and the impact on housing could be underestimated. We discuss it further in the next section.\n"
      ],
      "id": "c73e1505"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "dwelling_count_2023 = 3790210\n",
        "#https://assets.publishing.service.gov.uk/media/664da7e6993111924d9d395c/LT_100.ods\n",
        "dwelling_count_2024 = dwelling_count_2023 + 32162\n",
        "#net additional dwellings in 2023-2024. https://app.powerbi.com/view?r=eyJrIjoiZTE5YWQ3MDYtZmFjMC00N2YwLWIxM2EtYWY2NTk1NjExYjgwIiwidCI6ImJmMzQ2ODEwLTljN2QtNDNkZS1hODcyLTI0YTJlZjM5OTVhOCJ9\n",
        "\n",
        "print(f\"We have identified {len(airbnb_gdf_clean[0])} valid listings in 2023, and \"\n",
        "      f\"{len(airbnb_gdf_clean[1])} valid listings in 2024, with \"\n",
        "      f\"{((len(airbnb_gdf_clean[1]) - len(airbnb_gdf_clean[0])) / len(airbnb_gdf_clean[0])) * 100:.2f}% year-on-year increase.\")\n",
        "\n",
        "print(f\"The listings accounted for {len(airbnb_gdf_clean[0])/(dwelling_count_2023/100):.2f}% and {len(airbnb_gdf_clean[1])/(dwelling_count_2024/100):.2f}% of total dwellings respectively (Number for London).\")\n",
        "print(f\"Entire homes took up {len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['room_type'] == 'Entire home/apt']) / (dwelling_count_2023) * 100:.2f}% in 2023 and \"\n",
        "      f\"{len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['room_type'] == 'Entire home/apt']) / (dwelling_count_2024) * 100:.2f}% in 2024.\\n\")\n",
        "\n",
        "print(f\"In 2023, there were:\")\n",
        "print(airbnb_gdf_clean[0]['room_type'].value_counts())\n",
        "print(f\"{len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] == 1])} hosts with one listing, and {len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] > 1])} hosts with multiple listings.\")\n",
        "print(f\"{(len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] == 1]) / len(airbnb_gdf_clean[0])) * 100:.2f}% of hosts with one listing, and {(len(airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] > 1]) / len(airbnb_gdf_clean[0])) * 100:.2f}% of hosts with multiple listings.\")\n",
        "\n",
        "print(f\"\\nIn 2024, there were:\")\n",
        "print(airbnb_gdf_clean[1]['room_type'].value_counts())\n",
        "print(f\"{len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] == 1])} hosts with one listing, and {len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] > 1])} hosts with multiple listings.\")\n",
        "print(f\"{(len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] == 1]) / len(airbnb_gdf_clean[1])) * 100:.2f}% of hosts with one listing, and {(len(airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] > 1]) / len(airbnb_gdf_clean[1])) * 100:.2f}% of hosts with multiple listings.\")\n",
        "print(\"\\nDue to the relatively low counts of âotherâ room types we can discount these from our analysis and focus on âEntire Home/aptsâ and âPrivate roomsâ.\")"
      ],
      "id": "ba515f22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "airbnb_gdf_clean[0]['prvt_rms_in_cluster'] = pd.to_numeric(airbnb_gdf_clean[0]['prvt_rms_in_cluster'], errors='coerce')\n",
        "airbnb_gdf_clean[1]['prvt_rms_in_cluster'] = pd.to_numeric(airbnb_gdf_clean[1]['prvt_rms_in_cluster'], errors='coerce')\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the histogram for airbnb_gdf_clean[0] (2023 Listings)\n",
        "plt.hist(\n",
        "    airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] > 1][\"prvt_rms_in_cluster\"].dropna(), \n",
        "    bins=range(1, int(airbnb_gdf_clean[0]['prvt_rms_in_cluster'].max()) + 2),  # Ensure bins are integer\n",
        "    alpha=0.5, label='2023 Listings', color='blue', density=True\n",
        ")\n",
        "\n",
        "# Plot the histogram for airbnb_gdf_clean[1] (2024 Listings)\n",
        "plt.hist(\n",
        "    airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] > 1][\"prvt_rms_in_cluster\"].dropna(),\n",
        "    bins=range(1, int(airbnb_gdf_clean[1]['prvt_rms_in_cluster'].max()) + 2),  # Ensure bins are integer\n",
        "    alpha=0.5, label='2024 Listings', color='red', density=True\n",
        ")\n",
        "plt.xlabel('Number of Bedrooms')\n",
        "plt.ylabel('Share of Listings, %')\n",
        "plt.title('N of Bedrooms in Listings from Multi-Listing Hosts')\n",
        "\n",
        "plt.xticks(range(1, max(int(airbnb_gdf_clean[0]['prvt_rms_in_cluster'].max()), int(airbnb_gdf_clean[1]['beds'].max())) + 1))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"We can see that the listings from multi-listing hosts have more rooms in 2021.\")"
      ],
      "id": "c150e0cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "multi_listing_hosts_2023 = airbnb_gdf_clean[0][airbnb_gdf_clean[0]['new_host_listings_count'] > 1]\n",
        "multi_listing_hosts_2024 = airbnb_gdf_clean[1][airbnb_gdf_clean[1]['new_host_listings_count'] > 1]\n",
        "\n",
        "upper_limit = 10\n",
        "bins_ml_hosts = list(range(2, upper_limit + 1)) + [upper_limit + 1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.hist(\n",
        "    multi_listing_hosts_2023['new_host_listings_count'], bins=bins_ml_hosts,\n",
        "    alpha=0.5, label='2023 Listings', color='blue', density=True)\n",
        "\n",
        "plt.hist(\n",
        "    multi_listing_hosts_2024['new_host_listings_count'], bins=bins_ml_hosts,\n",
        "    alpha=0.5, label='2024 Listings', color='red', density=True)\n",
        "\n",
        "plt.xlabel('Number of Listings per Multi-Listing Host')\n",
        "plt.ylabel('Share of Multi-Listing Hosts')\n",
        "plt.title('N of Listings posted by Multi-Listing Hosts')\n",
        "\n",
        "plt.xticks(list(range(1, upper_limit + 1)) + [upper_limit + 1])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#___________________________________________________________________________________________________________________________________\n",
        "# the last bin shows all numbers over 10\n",
        "#___________________________________________________________________________________________________________________________________\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(f\"Average number of listings per multi-listing host in 2023: {multi_listing_hosts_2023['new_host_listings_count'].mean():.2f}\")\n",
        "print(f\"Average number of listings per multi-listing host in 2024: {multi_listing_hosts_2024['new_host_listings_count'].mean():.2f}\")\n",
        "print(\"23% of multi-listing hosts have only 2 listings in 2024. However, the rest have more, and the N of listings per multi-listing host increased from 14.8 to 19.5.\")"
      ],
      "id": "08e28ab0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "# ChatGPT was used here to add additional verification of matching geometry, This is because we are allowing the user to try other InsideAirbnb datasets which we didn't test\n",
        "def rel_prcnt_change_gdfs(gdf_old, gdf_new, cols):\n",
        "    \"\"\"\n",
        "    Function to find different in number of listings between 2 dates.\n",
        "    Args:\n",
        "    gdf_old: earlier airbnb dataset\n",
        "    gdf_new: latest airbnb dataset\n",
        "    cols: specify a subset of columns to be substracted\n",
        "    \"\"\"\n",
        "    #Verify that the geometry matches\n",
        "    if not gdf_old.geometry.equals(gdf_new.geometry):\n",
        "        raise ValueError(\"The geometries of the two GeoDataFrames do not match!\")\n",
        "    \n",
        "    #Verify that the specified columns exist in both GeoDataFrames\n",
        "    for col in cols:\n",
        "        if col not in gdf_old.columns or col not in gdf_new.columns:\n",
        "            raise ValueError(f\"Column '{col}' does not exist in both GeoDataFrames.\")\n",
        "    \n",
        "    #Create a copy of the new gdf to store the differences\n",
        "    gdf_diff = gdf_new.copy()\n",
        "    \n",
        "    # Subtract the specified columns\n",
        "    for col in cols:\n",
        "        gdf_diff[col] = ((gdf_new[col] - gdf_old[col]) / gdf_old[col])*100\n",
        "    \n",
        "    # Ensure the geometry column is preserved\n",
        "    gdf_diff['geometry'] = gdf_new['geometry']\n",
        "    \n",
        "    return gdf_diff"
      ],
      "id": "4a418fdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "cols_diff_airbnb = ['total_airbnbs', 'total_entireha', 'total_privateroom']\n",
        "diff2024_2023_boro = rel_prcnt_change_gdfs(airbnb_2023_boro[0], airbnb_2024_boro[0], cols_diff_airbnb)\n",
        "diff2024_2023_lsoa = rel_prcnt_change_gdfs(airbnb_2023_lsoa[0], airbnb_2024_lsoa[0], cols_diff_airbnb)"
      ],
      "id": "780e43d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "diff2024_2023_boro.head(2)"
      ],
      "id": "668fc678",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "diff2024_2023_boro[\"total_airbnbs\"].hist()"
      ],
      "id": "0441b495",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# ChatGPT was used here to plot in grid, and achieve the desired stye\n",
        "diff2024_2023_boro1 = pd.merge(\n",
        "    diff2024_2023_boro,  # Left DataFrame\n",
        "    df_CoreDemographics[['mnemonic', 'Total_Dwellings']],  # Right DataFrame with relevant columns\n",
        "    left_on='GSS_CODE',  # Column in diff2024_2023_boro\n",
        "    right_on='mnemonic',  # Column in df_CoreDemographics\n",
        "    how='left'  # Perform left join to keep all rows from diff2024_2023_boro\n",
        ")\n",
        "diff2024_2023_boro1[\"percent_of_totaldw_2023\"] = airbnb_2023_boro[0]['total_airbnbs']/ (diff2024_2023_boro1['Total_Dwellings']/100)\n",
        "diff2024_2023_boro1[\"percent_of_totaldw_2024\"] = airbnb_2024_boro[0]['total_airbnbs']/ (diff2024_2023_boro1['Total_Dwellings']/100)\n",
        "\n",
        "# Set the plot's limits for the color scale\n",
        "vmin = 0\n",
        "vmax = 2\n",
        "\n",
        "# Create the figure with two subplots (side-by-side)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Plot the first map for percent_of_totaldw_2023\n",
        "map_plot_2023 = diff2024_2023_boro1.plot(column='percent_of_totaldw_2023', ax=axes[0], legend=False,\n",
        "                                          cmap='magma_r', edgecolor='white', linewidth=0.8,\n",
        "                                          vmin=vmin, vmax=vmax)\n",
        "\n",
        "# Create the colorbar for the first plot (2023)\n",
        "sm_2023 = plt.cm.ScalarMappable(cmap='magma_r', norm=mpl.colors.Normalize(vmin=vmin, vmax=vmax))\n",
        "sm_2023._A = []  # Empty array to create the colorbar\n",
        "cbar_2023 = fig.colorbar(sm_2023, ax=axes[0], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.6)\n",
        "cbar_2023.set_label(\"Percent of Total Dwellings (%)\")\n",
        "cbar_2023.outline.set_visible(False)  # Remove border from the colorbar\n",
        "\n",
        "# Add labels (percent values) to the first map (2023)\n",
        "for idx, row in diff2024_2023_boro1.iterrows():\n",
        "    x, y = row.geometry.centroid.coords[0]\n",
        "    axes[0].annotate(f\"{row['percent_of_totaldw_2023']:.2f}%\", xy=(x, y), xytext=(3, 3), textcoords=\"offset points\",\n",
        "                     ha='center', fontsize=8, color='white', fontweight='bold')\n",
        "\n",
        "# Set title for the first map\n",
        "axes[0].set_title(\"Share of Homes Involved in Airbnb\\nin the Total Number of Dwellings (2023)\", fontsize=14, loc='left')\n",
        "\n",
        "# Plot the second map for percent_of_totaldw_2024\n",
        "map_plot_2024 = diff2024_2023_boro1.plot(column='percent_of_totaldw_2024', ax=axes[1], legend=False,\n",
        "                                          cmap='magma_r', edgecolor='white', linewidth=0.8,\n",
        "                                          vmin=vmin, vmax=vmax)\n",
        "\n",
        "# Create the colorbar for the second plot (2024)\n",
        "sm_2024 = plt.cm.ScalarMappable(cmap='magma_r', norm=mpl.colors.Normalize(vmin=vmin, vmax=vmax))\n",
        "sm_2024._A = []  # Empty array to create the colorbar\n",
        "cbar_2024 = fig.colorbar(sm_2024, ax=axes[1], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.6)\n",
        "cbar_2024.set_label(\"Percent of Total Dwellings (%)\")\n",
        "cbar_2024.outline.set_visible(False)  # Remove border from the colorbar\n",
        "\n",
        "# Add labels (percent values) to the second map (2024)\n",
        "for idx, row in diff2024_2023_boro1.iterrows():\n",
        "    x, y = row.geometry.centroid.coords[0]\n",
        "    axes[1].annotate(f\"{row['percent_of_totaldw_2024']:.2f}%\", xy=(x, y), xytext=(3, 3), textcoords=\"offset points\",\n",
        "                     ha='center', fontsize=8, color='white', fontweight='bold')\n",
        "\n",
        "# Set title for the second map\n",
        "axes[1].set_title(\"Share of Homes Involved in Airbnb\\nin the Total Number of Dwellings (2024)\", fontsize=14, loc='left')\n",
        "\n",
        "# Set axis limits and turn off axis\n",
        "for ax in axes:\n",
        "    ax.set_xlim([501000, 563000])\n",
        "    ax.set_ylim([155000, 202000])\n",
        "    ax.set_axis_off()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e5427cfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "# ChatGPT was used here to plot in grid, and achieve the desired stye\n",
        "# Ensure the columns exist and calculate vmin and vmax for both datasets\n",
        "vmin_boro = min(diff2024_2023_boro['total_entireha'].min(), \n",
        "                diff2024_2023_boro['total_privateroom'].min(), \n",
        "                diff2024_2023_boro['total_airbnbs'].min())\n",
        "vmax_boro = max(diff2024_2023_boro['total_entireha'].max(), \n",
        "                diff2024_2023_boro['total_privateroom'].max(), \n",
        "                diff2024_2023_boro['total_airbnbs'].max())\n",
        "\n",
        "vmin_lsoa = min(diff2024_2023_lsoa['total_entireha'].min(), \n",
        "                diff2024_2023_lsoa['total_privateroom'].min(), \n",
        "                diff2024_2023_lsoa['total_airbnbs'].min())\n",
        "vmax_lsoa = max(diff2024_2023_lsoa['total_entireha'].max(), \n",
        "                diff2024_2023_lsoa['total_privateroom'].max(), \n",
        "                diff2024_2023_lsoa['total_airbnbs'].max())\n",
        "\n",
        "# Create TwoSlopeNorm instances for custom normalization\n",
        "norm_boro = TwoSlopeNorm(vmin=-100, vcenter=0, vmax=100)\n",
        "norm_lsoa = TwoSlopeNorm(vmin=-100, vcenter=0, vmax=100)\n",
        "\n",
        "# Create the subplots (6 maps: 3 in a row)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Plot 'total_airbnbs' for Boroughs with custom normalization\n",
        "map1 = diff2024_2023_boro.plot(column='total_airbnbs', ax=axes[0, 0], legend=False,\n",
        "                                cmap='coolwarm', edgecolor='white', norm=norm_boro)\n",
        "axes[0, 0].set_axis_off()\n",
        "axes[0, 0].set_title(\"Relative % Difference\\nin Total Airbnb Listings\\nby Borough (2024-2023)\", fontsize=12, loc='left')\n",
        "cbar1 = fig.colorbar(map1.get_children()[0], ax=axes[0, 0], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.5)\n",
        "cbar1.set_label(\"Relative % Difference\", fontsize=10)\n",
        "cbar1.outline.set_visible(False)\n",
        "\n",
        "# Plot 'total_entireha' for Boroughs with custom normalization\n",
        "map2 = diff2024_2023_boro.plot(column='total_entireha', ax=axes[0, 1], legend=False, cmap='coolwarm', edgecolor='white', norm=norm_boro)\n",
        "axes[0, 1].set_axis_off()\n",
        "axes[0, 1].set_title(\"Relative % Difference\\nin Entire Home Listings\\nby Borough (2024-2023)\", fontsize=12, loc='left')\n",
        "cbar2 = fig.colorbar(map2.get_children()[0], ax=axes[0, 1], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.5)\n",
        "cbar2.set_label(\"Relative % Difference\", fontsize=10)\n",
        "cbar2.outline.set_visible(False)\n",
        "\n",
        "# Plot 'total_privateroom' for Boroughs with custom normalization\n",
        "map3 = diff2024_2023_boro.plot(column='total_privateroom', ax=axes[0, 2], legend=False, cmap='coolwarm', edgecolor='white', norm=norm_boro)\n",
        "axes[0, 2].set_axis_off()\n",
        "axes[0, 2].set_title(\"Relative % Difference\\nin Private Room Listings\\nby Borough (2024-2023)\", fontsize=12, loc='left')\n",
        "cbar3 = fig.colorbar(map3.get_children()[0], ax=axes[0, 2], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.5)\n",
        "cbar3.set_label(\"Relative % Difference\", fontsize=10)\n",
        "cbar3.outline.set_visible(False)\n",
        "\n",
        "# Plot 'total_airbnbs' for LSOAs with custom normalization\n",
        "map4 = diff2024_2023_lsoa.plot(column='total_airbnbs', ax=axes[1, 0], legend=False, cmap='coolwarm', edgecolor='white', linewidth=0.2, norm=norm_lsoa)\n",
        "axes[1, 0].set_axis_off()\n",
        "axes[1, 0].set_title(\"Relative % Difference\\nin Total Airbnb Listings\\nby LSOA (2024-2023)\", fontsize=12, loc='left')\n",
        "cbar4 = fig.colorbar(map4.get_children()[0], ax=axes[1, 0], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.5)\n",
        "cbar4.set_label(\"Relative % Difference\", fontsize=10)\n",
        "cbar4.outline.set_visible(False)\n",
        "\n",
        "# Plot 'total_entireha' for LSOAs with custom normalization\n",
        "map5 = diff2024_2023_lsoa.plot(column='total_entireha', ax=axes[1, 1], legend=False,cmap='coolwarm', edgecolor='white', linewidth=0.2, norm=norm_lsoa)\n",
        "axes[1, 1].set_axis_off()\n",
        "axes[1, 1].set_title(\"Relative % Difference\\nin Entire Home Listings\\nby LSOA (2024-2023)\", fontsize=12, loc='left')\n",
        "cbar5 = fig.colorbar(map5.get_children()[0], ax=axes[1, 1], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.5)\n",
        "cbar5.set_label(\"Relative % Difference\", fontsize=10)\n",
        "cbar5.outline.set_visible(False)\n",
        "\n",
        "# Plot 'total_privateroom' for LSOAs with custom normalization\n",
        "map6 = diff2024_2023_lsoa.plot(column='total_privateroom', ax=axes[1, 2], legend=False,cmap='coolwarm', edgecolor='white', linewidth=0.2, norm=norm_lsoa)\n",
        "axes[1, 2].set_axis_off()\n",
        "axes[1, 2].set_title(\"Relative % Difference\\nin Private Room Listings\\nby LSOA (2024-2023)\", fontsize=12, loc='left')\n",
        "cbar6 = fig.colorbar(map6.get_children()[0], ax=axes[1, 2], orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.5)\n",
        "cbar6.set_label(\"Relative % Difference\", fontsize=10)\n",
        "cbar6.outline.set_visible(False)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(hspace=-0.3, wspace=0.2)  # Adjust gaps between rows and columns\n",
        "plt.show()"
      ],
      "id": "888e5762",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# #| include: false\n",
        "# ChatGPT was used here to plot in grid, and achieve the desired stye\n",
        "# This map is high resolution and therefore commented out. Run this cell if you want to explore localised patterns with Openstreetmap underlay\n",
        "\n",
        "# # Ensure the LSOA data has the necessary 'total_entireha' column and compute the relative % difference\n",
        "# # Calculate the vmin and vmax values for custom normalization\n",
        "# vmin_lsoa = -100\n",
        "# vmax_lsoa = 100\n",
        "\n",
        "# # Create TwoSlopeNorm instance for custom normalization\n",
        "# norm_lsoa = TwoSlopeNorm(vmin=-100, vcenter=0, vmax=100)\n",
        "\n",
        "# # Create the plot for the LSOA map with relative % difference in 'total_entireha'\n",
        "# fig, ax = plt.subplots(1, 1, figsize=(15, 10), dpi=300)\n",
        "\n",
        "# # Plot 'total_entireha' for LSOAs with custom normalization\n",
        "# map_plot = diff2024_2023_lsoa.plot(column='total_entireha', ax=ax, legend=False,\n",
        "#                                    cmap='coolwarm', edgecolor='white', linewidth=0.2, norm=norm_lsoa, alpha=0.7)\n",
        "\n",
        "# # Add OpenStreetMap background using contextily\n",
        "# ctx.add_basemap(ax, crs=diff2024_2023_lsoa.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, zoom=12)\n",
        "\n",
        "# # Add colorbar for the map\n",
        "# cbar = fig.colorbar(map_plot.get_children()[0], ax=ax, orientation=\"vertical\", fraction=0.02, pad=0.02, shrink=0.6)\n",
        "# cbar.set_label(\"Relative % Difference\", fontsize=10)\n",
        "# cbar.outline.set_visible(False)  # Remove border from the colorbar\n",
        "\n",
        "# # Set title for the map\n",
        "# ax.set_title(\"Relative % Difference\\nin Entire Home Listings\\nby LSOA (2024-2023)\", fontsize=12, loc='left')\n",
        "\n",
        "# # Set axis limits and remove axes\n",
        "# ax.set_axis_off()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "id": "55496351",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# #| echo: false\n",
        "# # write the data to be used in further analysis (ONLY IF NEEDED. CODE WORKS WITHOUT THIS)\n",
        "# fn = \"borough_counts.csv\"\n",
        "# path = os.path.join('data','clean')\n",
        "\n",
        "# if not os.path.exists(path):\n",
        "#     print(f\"Creating {path} under {os.getcwd()}\")\n",
        "#     os.makedirs(path)\n",
        "    \n",
        "# boro_count_gdf.to_csv(os.path.join(path,fn), index=False)\n",
        "# #boro_count_gdf.to_file(\"boro_count.geoparquet\")\n",
        "# print(\"Done.\")\n",
        "\n",
        "# fn = \"lsoa_counts.csv\"\n",
        "# path = os.path.join('data','clean')\n",
        "\n",
        "# if not os.path.exists(path):\n",
        "#     print(f\"Creating {path} under {os.getcwd()}\")\n",
        "#     os.makedirs(path)\n",
        "    \n",
        "# lsoa_count_gdf.to_csv(os.path.join(path,fn), index=False)\n",
        "# #lsoa_count_gdf.to_file(\"lsoa_count.geoparquet\")\n",
        "# print(\"Done.\")"
      ],
      "id": "c3f8d04f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? \n",
        "\n",
        "### Potential link between STL and housing crisis in London\n",
        "Despite a narrowing rent-wage gap, rising living costs force low-income Londoners to spend up to 77% of wages on housing. Higher mortgage rates further exclude many from homeownership, disproportionately impacting vulnerable populations, leading to poverty, evictions, overcrowding, or homelessness [@EY_Airbnb_2024, @scanlon2024, @finnerty2024, @otte2024, @builtassetmanagement2024, @londonassembly2024, @ageuk2024, @kellyk2024, @crisis2024, @bevan2023]. There are many claims that STLs exacerbate this housing crisis, pushing the demand down to the social and private rental sector.\n",
        "In February 2024, the UK government announced reforms requiring planning permission for STLs exceeding 90 nights, and a mandatory STL registration scheme [@govuk2024] that will close the STL data availability gap, discussed in Q4. Additionally, the furnished holiday lettings tax regime will be abolished [@govuk_abolition2024]. These continuous efforts aim to keep homes available for local communities.\n",
        "Airbnb claims no link between listing growth and rising housing costs, backed by studies discussed in Q6. A caveat of published studies is their UK-wide statistics for some parameters, which may not reflect Londonâs heated STL market. With Airbnb comprising 47% of European platform STL supply and 30% across all types, STL impacts on housing may be more significant [@lighthouse2021, @laymyhat].\n",
        "### Our research\n",
        "We base our research on the independent studies conducted in 2019 and 2020 [@Shabrina2021, @Todd2021]. Our goal is to identify proxies accounting for the impacts of the Airbnb listings on housing in London. We utilise the methodology outlined in the articles above.\n",
        "### Datasets:\n",
        "* InsideAirbnb - see Q6 for details.\n",
        "* London Boroughs' administrative boundaries provide a foundation for decision-makers to conduct further analysis.\n",
        "* LSOA 2021 represent smaller geographies. Issues include potential misrepresentation of demographics, exclusion of daytime populations, and being too granular for effective citywide visualisation.\n",
        "* Census 2021 provides the most accurate population data, but results may be skewed due to limited mobility and migration during the Covid-19 pandemic.\n",
        "\n",
        "### Results and interpretation:\n",
        "\n",
        "The model explains about 53% of the variation in rental prices (RÂ²), offering a solid understanding of the main factors influencing rent changes.\n"
      ],
      "id": "2feffc18"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#___________________ extracting data needed for regression _______________\n",
        "# Extracting the percentage and the total number of vacant properties in 2023\n",
        "For_regressionV = df_CoreDemographics[['VacantPercent','2023_LTVacant', 'LAName', 'mnemonic']]\n",
        "\n",
        "# Display the first few rows to ensure it worked correctly\n",
        "print(For_regressionV.head())\n",
        "\n",
        "#print(df_rentalprice) \n",
        "\n",
        "# Extracting Annual change column because we want to observe how the number of listings may influence the change in rental price\n",
        "For_RegressionR = df_rentalprice[['mnemonic', 'Area name', 'Annual change']]\n",
        "\n",
        "#print(For_RegressionR)\n",
        "\n",
        "# ______________________ calculating the distance to the centre _________\n",
        "\n",
        "# defining latitudes and longitude finder from london uk \n",
        "lat_lon = [\n",
        "    (51.5014, -0.1419),  # Buckingham Palace\n",
        "    (51.5081, -0.0760),  # Tower of London\n",
        "    (51.5194, -0.1269),  # British Museum\n",
        "    (51.5007, -0.1246),  # Big Ben\n",
        "    (51.5033, -0.1195)   # London Eye\n",
        "]\n",
        "\n",
        "# Reprojecting coordinates to BNG (EPSG:27700)\n",
        "transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:27700\", always_xy=True)\n",
        "eastings_northings = [transformer.transform(lon, lat) for lat, lon in lat_lon]\n",
        "\n",
        "# Calculating midpoint\n",
        "eastings, northings = zip(*eastings_northings)\n",
        "midpoint = (np.mean(eastings), np.mean(northings))\n",
        "\n",
        "\n",
        "# Calculating centroids of each borough and distance to midpoint\n",
        "boro_gdf[\"centroid\"] = boro_gdf.geometry.centroid\n",
        "boro_gdf[\"distance_to_midpoint\"] = boro_gdf[\"centroid\"].apply(\n",
        "    lambda point: np.sqrt((point.x - midpoint[0])**2 + (point.y - midpoint[1])**2)\n",
        ")\n",
        "\n",
        "\n",
        "# Display only the NAME and distance_to_midpoint columns\n",
        "#print(boro_gdf[['GSS_CODE', 'distance_to_midpoint', 'geometry']])\n",
        "\n",
        "#print(diff2024_2023_boro)"
      ],
      "id": "0ca39d65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "\n",
        "#shapiro wilk test to see if the data is normally distributed\n",
        "#-------  on the listings data ---------\n",
        "\n",
        "# Perform the Shapiro-Wilk test\n",
        "stat, p_value = shapiro(diff2024_2023_boro[\"total_airbnbs\"])\n",
        "\n",
        "# Display the results\n",
        "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "\n",
        "#the W values 0.7... is less than 1 (suggesting that the data is not normally distributed) and the p value 0.00000505 is <0.05\n",
        "#therefore the null hypothesis which assumes that the data is normally distributed is rejected \n",
        "\n",
        "\n",
        "\n",
        "# ----------- Perform the test on the '2023' column ------ #\n",
        "stat, p_value = shapiro(For_regressionV[\"VacantPercent\"])\n",
        "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "#Interpretation \n",
        "#0.9465112270729066 W value close to one so its close to normal \n",
        "#p value is 0.10520533110791175 greater than 0.05 so we can fail to reject the null hypothesis as there is no significant evidence\n",
        "#that the data is not normally distributes\n",
        "\n",
        "\n",
        "\n",
        "# Create Q-Q plot\n",
        "stats.probplot(For_regressionV[\"VacantPercent\"], dist=\"norm\", plot=plt)\n",
        "plt.show()\n",
        "\n",
        "#hist to actually visualise the distribution to determine whether we transform or not\n",
        "# Histogram with a density line\n",
        "sns.histplot(For_regressionV[\"VacantPercent\"], kde=True, color=\"blue\", bins=20)\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('vacant properties')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Histogram vacant properties')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "#as you can see it is still a bit skewed to the right\n",
        "\n",
        "\n",
        "\n",
        "#Now we'll do one for the rental price\n",
        "stat, p_value = shapiro(For_RegressionR['Annual change'])\n",
        "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "#owned\n",
        "stat, p_value = shapiro(boro_gdf['distance_to_midpoint'])\n",
        "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "\n",
        "# checking ifits skewed\n",
        "from scipy.stats import skew\n",
        "\n",
        "skew_vac = skew(For_regressionV['VacantPercent'])\n",
        "\n",
        "\n",
        "print(f\"Skewness of Rented: {skew_vac}\")"
      ],
      "id": "63d1dd33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "\n",
        "# Histograms for all variables to visualise distribution \n",
        "# Histogram for \"total_airbnbs\"\n",
        "sns.histplot(diff2024_2023_boro[\"total_airbnbs\"], kde=True, color=\"blue\", bins=20)\n",
        "plt.xlabel('Total Airbnbs')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Histogram of Total Airbnbs')\n",
        "plt.show()\n",
        "\n",
        "# Histogram for \"VacantPercent\"\n",
        "sns.histplot(For_regressionV[\"VacantPercent\"], kde=True, color=\"green\", bins=20)\n",
        "plt.xlabel('Vacant Percent')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Histogram of Vacant Percent')\n",
        "plt.show()\n",
        "\n",
        "# Histogram for \"Annual Change\"\n",
        "sns.histplot(For_RegressionR['Annual change'], kde=True, color=\"orange\", bins=20)\n",
        "plt.xlabel('Annual Change')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Histogram of Annual Change')\n",
        "plt.show()\n",
        "\n",
        "# Histogram for \"distance_to_midpoint\"\n",
        "sns.histplot(boro_gdf['distance_to_midpoint'], kde=True, color=\"purple\", bins=20)\n",
        "plt.xlabel('Distance to Midpoint')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Histogram of Distance to Midpoint')\n",
        "plt.show()"
      ],
      "id": "4db3da10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "\n",
        "#______ ________ merging the data for the regression model __________\n",
        "\n",
        "\n",
        "\n",
        "# Merge using different column names\n",
        "vacantprops_airbnb = pd.merge(For_regressionV[['mnemonic', 'LAName', 'VacantPercent']],  \n",
        "                                   diff2024_2023_boro[['GSS_CODE', 'total_airbnbs']], \n",
        "                                   left_on='mnemonic',  \n",
        "                                   right_on='GSS_CODE',  \n",
        "                                   how='left', suffixes=('', '_dup'))\n",
        "\n",
        "\n",
        "# Print the first few rows of the merged DataFrame\n",
        "#print(vacantprops_airbnb.head())\n",
        "\n",
        "\n",
        "#Adding rents\n",
        "Adding_more = pd.merge(\n",
        "    vacantprops_airbnb, \n",
        "    For_RegressionR[['mnemonic', 'Annual change']], \n",
        "    left_on='mnemonic',  # Merge on LAcode from vacantprops_with_tenure\n",
        "    right_on='mnemonic',  # Merge on GSS_CODE from merged_borough_counts\n",
        "    how='inner'  # Inner join to only keep matching rows\n",
        ")\n",
        "\n",
        "# Merging the result with boro to add Distance to Midpoint\n",
        "Regression_Final = pd.merge(\n",
        "    Adding_more, \n",
        "    boro_gdf[['GSS_CODE', 'distance_to_midpoint', 'geometry']], \n",
        "    on='GSS_CODE',  # Merge on GSS_CODE which is common\n",
        "    how='inner'  # Inner join to keep only matching rows\n",
        ")\n",
        "\n",
        "# Display the final merged dataframe\n",
        "#print(Regression_Final)\n",
        "\n",
        "\n",
        "# Removing duplicate columns if any\n",
        "Regression_Final = Regression_Final.loc[:, ~Regression_Final.columns.duplicated()]\n",
        "\n",
        "\n",
        "# Dropping the redundant 'mnemonic' column after merging\n",
        "Regression_Final1 = Regression_Final.drop(columns=['mnemonic'], errors='ignore')\n",
        "\n",
        "# Print the final DataFrame\n",
        "#print(Regression_Final1.head())\n",
        "\n",
        "#______ grouping the rows by borough ______\n",
        "\n",
        "\n",
        "# Group by borough and aggregate\n",
        "Regression_Final1 = Regression_Final.groupby('LAName').agg({\n",
        "    'VacantPercent': 'mean',\n",
        "    'total_airbnbs': 'mean',\n",
        "    'Annual change': 'mean',\n",
        "    'distance_to_midpoint': 'mean',\n",
        "}).reset_index()\n",
        "\n",
        "# Combine geometries for each group\n",
        "geometry = Regression_Final.groupby('LAName')['geometry'].apply(lambda g: unary_union(g)).reset_index()\n",
        "\n",
        "# Merge geometries back into the grouped data\n",
        "Regression_Final1 = Regression_Final1.merge(geometry, on='LAName')\n",
        "\n",
        "# Convert to GeoDataFrame\n",
        "Regression_Final1 = gpd.GeoDataFrame(Regression_Final1, geometry='geometry')\n",
        "\n",
        "# Convert distance to kilometers\n",
        "Regression_Final1['distance_to_midpoint_km'] = Regression_Final1['distance_to_midpoint'] / 1000\n",
        "\n",
        "print(Regression_Final1.head())\n",
        "\n",
        "\n",
        "\n",
        "Regression_Final1.loc[:, 'distance_to_midpoint_km'] = Regression_Final1['distance_to_midpoint'] / 1000\n",
        "#the scale should not dominate others numerically which is why the distance to the centre is being calculated in km\n",
        "\n",
        "#Viewing the new column to ensure the operation worked\n",
        "Regression_Final1[['distance_to_midpoint', 'distance_to_midpoint_km']].head()"
      ],
      "id": "8cb904f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "# __________________ Running the regression model ____________\n",
        "\n",
        "\n",
        "# Running the first regression model on the data\n",
        "X = Regression_Final1[['total_airbnbs','VacantPercent','distance_to_midpoint_km']]  \n",
        "y = Regression_Final1['Annual change']  \n",
        "\n",
        "\n",
        "# Fitting the model\n",
        "model = sm.OLS(y, sm.add_constant(X)).fit()\n",
        "\n",
        "# Displaying model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Plot residuals after the transformation\n",
        "residuals = model.resid\n",
        "fitted_values = model.fittedvalues\n",
        "\n",
        "# ______________ Residuals vs Fitted plot\n",
        "'''\n",
        "plt.scatter(fitted_values, residuals)\n",
        "plt.axhline(0, color='black', linestyle='--')\n",
        "plt.title(\"Residuals vs Fitted Values\")\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "'''\n",
        "# QQ plot\n",
        "#sm.qqplot(residuals, line='45')\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "# Breusch-Pagan test for heteroskedasticity\n",
        "_, p_value, _, _ = sms.het_breuschpagan(residuals, model.model.exog)\n",
        "#print(f'Breusch-Pagan p-value: {p_value}')\n",
        "\n",
        "from statsmodels.stats.diagnostic import het_white\n",
        "\n",
        "# White test for heteroskedasticity\n",
        "_, p_value, _, _ = het_white(residuals, model.model.exog)\n",
        "#print(f'White test p-value: {p_value}')"
      ],
      "id": "08d8bed5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "# Extracting regression components\n",
        "coefficients = model.params.values[1:]  # Exclude intercept\n",
        "errors = model.bse.values[1:]           # Standard errors (excluding intercept)\n",
        "coef_names = X.columns.tolist()         # Predictor variable names\n",
        "\n",
        "# Print extracted values for confirmation\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Standard Errors:\", errors)\n",
        "print(\"Predictor Names:\", coef_names)"
      ],
      "id": "c278fadf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# #| echo: false\n",
        "\n",
        "\n",
        "\n",
        "# Custom color palette\n",
        "gradient_colors = [\"#B39DDB\", \"#FFAB91\", \"#81D4FA\"]  # Light Purple, Light Orange, Light Blue\n",
        "\n",
        "# Updated coefficient names for correct alignment\n",
        "coef_names = [\"% change in Airbnbs\", \"Vacant properties (%)\", \"D. to centre (km)\"]\n",
        "\n",
        "# Set up plot style\n",
        "sns.set(style=\"white\", font_scale=1.1)\n",
        "\n",
        "# Create a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# ----------------- Plot 1: Residuals vs Fitted Values -----------------\n",
        "axes[0, 0].scatter(fitted_values, residuals, color=gradient_colors[0], alpha=0.7, edgecolor=\"white\", s=50)\n",
        "axes[0, 0].axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
        "axes[0, 0].set_title(\"Residuals vs Fitted Values\", fontsize=13, fontweight=\"bold\")\n",
        "axes[0, 0].set_xlabel(\"Fitted Values\")\n",
        "axes[0, 0].set_ylabel(\"Residuals\")\n",
        "\n",
        "# ----------------- Plot 2: Histogram of Residuals -----------------\n",
        "sns.histplot(\n",
        "    residuals,\n",
        "    kde=True,  # Add KDE curve\n",
        "    ax=axes[0, 1],\n",
        "    color=gradient_colors[1],  # Histogram bars\n",
        "    edgecolor=\"black\",\n",
        "    bins=20,\n",
        "    line_kws={\"color\": \"black\", \"linewidth\": 2.5}  # Style KDE line (black and bold)\n",
        ")\n",
        "axes[0, 1].set_title(\"Distribution of Residuals\", fontsize=13, fontweight=\"bold\")\n",
        "axes[0, 1].set_xlabel(\"Residuals\")\n",
        "axes[0, 1].set_ylabel(\"Density\")\n",
        "\n",
        "# ----------------- Plot 3: Q-Q Plot -----------------\n",
        "qq = sm.qqplot(\n",
        "    residuals,\n",
        "    line=\"45\",\n",
        "    ax=axes[1, 0],\n",
        "    markerfacecolor=\"#87CEEB\",  # Light blue markers\n",
        "    markeredgecolor=\"None\",\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# Style the 45-degree reference line\n",
        "for line in axes[1, 0].lines:\n",
        "    if line.get_linestyle() == \"-\":  \n",
        "        line.set_color(\"black\")      \n",
        "        line.set_linestyle(\"--\")\n",
        "        line.set_linewidth(2)\n",
        "        break\n",
        "\n",
        "axes[1, 0].set_title(\"Q-Q Plot of Residuals\", fontsize=13, fontweight=\"bold\")\n",
        "\n",
        "# ----------------- Plot 4: Regression Coefficients -----------------\n",
        "bars = axes[1, 1].barh(coef_names[::-1], coefficients[::-1], xerr=errors[::-1], \n",
        "                       color=gradient_colors, edgecolor=\"black\", capsize=5)  # Reverse for alignment\n",
        "axes[1, 1].axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
        "axes[1, 1].set_title(\"Regression Coefficients with 95% Confidence Intervals\", fontsize=13, fontweight=\"bold\")\n",
        "axes[1, 1].set_xlabel(\"Coefficient Value\")\n",
        "\n",
        "# Adjust label positions to prevent overlap\n",
        "for bar, coef in zip(bars, coefficients[::-1]):\n",
        "    width = bar.get_width()\n",
        "    x_position = width + (0.02 if width > 0 else -0.02)\n",
        "    alignment = 'left' if width > 0 else 'right'\n",
        "    y_position = bar.get_y() + bar.get_height() / 2\n",
        "    axes[1, 1].text(x_position, y_position, f\"{width:.3f}\", \n",
        "                    va='center', ha=alignment, fontsize=10, color=\"black\", \n",
        "                    bbox=dict(facecolor=\"white\", edgecolor=\"none\", boxstyle=\"round,pad=0.2\"))\n",
        "\n",
        "# ----------------- Layout and Final Touches -----------------\n",
        "plt.tight_layout(pad=2.5)\n",
        "plt.suptitle(\"OLS Regression Diagnostics\", fontsize=16, fontweight=\"bold\", y=1.02)\n",
        "plt.show()"
      ],
      "id": "a6fea129",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "\n",
        "#checking the VIF \n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Add constant to the independent variables for VIF calculation\n",
        "X_with_const = add_constant(X)\n",
        "\n",
        "# Calculate VIF for each independent variable\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = X_with_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
        "\n",
        "# Display VIF values\n",
        "print(vif_data)\n",
        "\n",
        "#No signs of multicolinearity "
      ],
      "id": "8c814aa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model suggests that for every 1% increase in the number of Airbnb listings, the change in rental prices grew by 0.065%. In areas where Airbnb listings would have doubled, featuring 100% increase, rents would have risen by an average of 6.5%. For tenants, this results in significant price increases: in central London, where average rents are around Â£2,121 per month, a 6.5% rise would add approximately Â£138 per month, or Â£1,656 per year. \n",
        "\n",
        "Apart from Airbnb impact, the model results also show that neighborhoods closer to the centre see an additional 0.34% increase in rental prices for every kilometer closer. However, the percentage of vacant dwellings showed little connection to rent changes. \n",
        "\n",
        "The story doesnât end with just the numbers. The analysis also revealed that changes in rental prices are not evenly spread across the city. Tests on the residuals (errors) displayed uneven patterns in the data, and further spatial analysis confirmed significant clustering of residuals. \n"
      ],
      "id": "8085c3d2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "#once I grouped the data by boroughs I realised it was not really showing anything\n",
        "\n",
        "# ____________ checking morans I for any spatial autocorellation ____\n",
        "\n",
        "#print(type(Regression_Final1))\n",
        "\n",
        "\n",
        "# needs to be a geopandas data frame \n",
        "Rgdf = gpd.GeoDataFrame(Regression_Final1, geometry='geometry') #Rgdf - residuals gdf\n",
        "#print(type(Rgdf))\n",
        "\n",
        "# Adding residuals to the GeoDataFrame\n",
        "Rgdf['residuals'] = residuals\n",
        "\n",
        "\n",
        "Rgdf = Rgdf.set_crs(epsg=27700)\n",
        "\n",
        "#print(Rgdf.crs)\n",
        "\n",
        "\n",
        "from esda import Moran_Local\n",
        "\n",
        "# ___________ Morans I ____________\n",
        "\n",
        "# Queen Weights\n",
        "w = Queen.from_dataframe(Rgdf, use_index=True)\n",
        "w.transform = 'R'\n",
        "\n",
        "\n",
        "mi = esda.Moran(Rgdf['residuals'], w)\n",
        "print(f\"{mi.I:0.4f}\")\n",
        "print(f\"{mi.p_sim:0.4f}\")\n",
        "\n",
        "#moran_scatterplot(mi)\n",
        "\n",
        "# ________ local Morans I on the residuals ___________\n",
        "\n",
        "lisa = esda.Moran_Local(Rgdf.residuals, w)\n",
        "# Break observations into significant or not\n",
        "Rgdf['sig'] = lisa.p_sim < 0.05\n",
        "# Store the quadrant they belong to\n",
        "Rgdf['quad'] = lisa.q\n",
        "Rgdf[['LAName','residuals','sig','quad']].sample(5, random_state=42)\n",
        "\n",
        "#print(Rgdf)\n",
        "\n",
        "plot_local_autocorrelation(lisa, Rgdf, 'residuals')"
      ],
      "id": "7c5c64bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# #| echo: false\n",
        "\n",
        "# Set normalization for residuals\n",
        "vmin = Rgdf['residuals'].min()\n",
        "vmax = Rgdf['residuals'].max()\n",
        "norm = TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
        "\n",
        "# Plot the residuals with axes and consistent colorbar\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "# Choropleth map\n",
        "Rgdf.plot(\n",
        "    column='residuals',\n",
        "    cmap='coolwarm',\n",
        "    norm=norm,\n",
        "    edgecolor=\"black\",\n",
        "    linewidth=0.5,\n",
        "    legend=True,\n",
        "    legend_kwds={'label': \"Residual Value\", 'orientation': \"horizontal\"},\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "# Add axes labels\n",
        "ax.set_title(\"Spatial Distribution of Residuals\", fontsize=12, fontweight=\"bold\")\n",
        "ax.set_xlabel(\"Longitude\", fontsize=10)\n",
        "ax.set_ylabel(\"Latitude\", fontsize=10)\n",
        "\n",
        "# Force proper axis limits for clean layout\n",
        "ax.set_xlim(Rgdf.total_bounds[0], Rgdf.total_bounds[2])\n",
        "ax.set_ylim(Rgdf.total_bounds[1], Rgdf.total_bounds[3])\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "42d067c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To build on this analysis, future work could explore how neighborhoods influence each other by using tools that better capture these spatial relationships, including Spatial Lag or Spatial Error. Adding more information, including income levels and housing availability, could also help explain why rent changes happen in some places but not others. Additionally, a more robust regression model can be undertaken that accounts for the uneven spread of residuals.\n",
        "\n",
        "### Potential analysis expansion:\n",
        "Below are the potential additional variables that could be tested in the future in multiple linear regression:\n",
        "* Social housing waiting times;\n",
        "* No-fault evictions;\n",
        "* Total new rough sleepers;\n",
        "* Average long-term room rental prices (e.g. from Spareroom);\n",
        "* % or N of Houses of Multiple Occupancy;\n",
        "* % or N of flatshare households;\n",
        "* N of Property guardians;\n",
        "* N of rooms in airbnb compared to n of homes on Airbnb.\n",
        "\n",
        "### Implications to the regulation of STL in London:\n",
        "**Promote casual STLs**\n",
        "Following the EDA, enforcing the 90-day rule remains crucial. Multi-listing hosts with large portfolios should be closely monitored for compliance, as they may duplicate listings to bypass platform restrictions. A centralized STL registry, as proposed in February 2024, would enhance transparency, ensure compliance, enable local councils to monitor STL density and trends, and effectively distinguish between casual and professional hosts.\n",
        "\n",
        "**Encourage private rooms not entire homes**\n",
        "Given the high concentration of listings, increased pressure on the rental market, and the 0.34% rent increase per kilometer closer to the city center, we propose implementing zoning restrictions in high-demand central boroughs such as Westminster, Camden, and Kensington & Chelsea, and in other areas based on rental market vulnerability, housing supply shortages, and local socioeconomic conditions. We suggest using spatial models (e.g., Spatial Lag/Error) to predict areas at risk of STL saturation and proactively apply regulation. These measures could cap the total number of listings or limit entire home rentals to encourage private room listings or overall reduction of the number of listings.\n",
        "\n",
        "**Address-based restrictions**\n",
        "The limitations of anonymized data restrict analysis of property types used for short-term lets. Once the STL registry is operational, regulations could target specific addresses, such as affordable housing, social housing, and newly built properties, to preserve their intended purpose of addressing housing needs rather than catering to tourists.\n",
        "\n",
        "**Redirect STL-generated taxes or fines**\n",
        "There is a potential to address declining homeownership through redirecting STL taxes or fines into affordable housing initiatives, supporting vulnerable renters and first-time buyers. Additionally, introducing an STL tax or levy in these areas could generate revenue to support affordable housing and community programs.\n",
        "\n",
        "**Long-term rent should be made more profitable**\n",
        "Greater support is needed to encourage property owners to prioritize long-term rentals over short-term lets. Currently, STLs are widely promoted as the most profitable option. Policy solutions could include requiring hosts and platforms to conduct Social Impact Assessments to evaluate STL impacts on local housing markets during planning approvals. Additionally, adjustments to the tax regime for long-term rentals, funded through redirected STL taxes, could provide further incentives.\n",
        "\n",
        "**The landlord should have a choice**\n",
        "Under the new rules, homeowners will be able to let their primary residence for up to 90 nights annually without planning permission, and dedicated short-term lets will be automatically reclassified into a new use class. This approach does not provide landlords sufficient time to assess the revenue implications, particularly if proposed incentives for long-term rentals are introduced. The government should reconsider the automatic reclassification of short-term lets.\n",
        "\n",
        "## References"
      ],
      "id": "6bc0f02d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "jupytext": {
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.16.4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}